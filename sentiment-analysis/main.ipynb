{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96364735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f604b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Define a single flat configuration object for all hyperparameters\n",
    "config = {\n",
    "    # Random seed configuration\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Model configuration\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"model_num_labels\": 2,\n",
    "    \n",
    "    # Dataset configuration\n",
    "    \"dataset_name\": \"stanfordnlp/sst2\",\n",
    "    \"train_split_ratio\": 0.7,\n",
    "    \"validation_split_ratio\": 0.15,\n",
    "    \"dev_split_ratio\": 0.1,\n",
    "    \"test_split_ratio\": 0.05,\n",
    "    \"train_subset_size\": 200,\n",
    "    \"validation_subset_size\": 50,\n",
    "    \"dev_subset_size\": 50,\n",
    "    \"test_subset_size\": 50,\n",
    "    \"num_proc\": 4,\n",
    "    \n",
    "    # Tokenizer configuration\n",
    "    \"tokenizer_padding\": \"max_length\",\n",
    "    \"tokenizer_truncation\": True,\n",
    "    \"tokenizer_max_length\": 512,\n",
    "    \n",
    "    # Training configuration\n",
    "    \"output_dir\": \"./results\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"num_epochs\": 2,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"use_mps_device\": torch.backends.mps.is_available() and torch.backends.mps.is_built(),\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4d751cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All random seeds have been set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Define a single seed value to use throughout the code\n",
    "SEED = config[\"seed\"]\n",
    "\n",
    "# Set seeds for Python's random module\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seed for Hugging Face Transformers\n",
    "set_seed(SEED)\n",
    "\n",
    "# For some operations on Apple Silicon (MPS)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "print(f\"All random seeds have been set to {SEED} for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85142507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the DistilBERT model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(config[\"model_name\"])\n",
    "model = DistilBertForSequenceClassification.from_pretrained(config[\"model_name\"], num_labels=config[\"model_num_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c089f527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh instance of the base DistilBERT model\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the base model and tokenizer (will not use your fine-tuned weights)\n",
    "base_tokenizer = DistilBertTokenizer.from_pretrained(config[\"model_name\"])\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(config[\"model_name\"], num_labels=config[\"model_num_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44bf48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(config[\"dataset_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a81d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "Dataset type: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "Number of examples per split:\n",
      "  train: 67349\n",
      "  validation: 872\n",
      "  test: 1821\n",
      "\n",
      "Features in the dataset:\n",
      "  idx: Value(dtype='int32', id=None)\n",
      "  sentence: Value(dtype='string', id=None)\n",
      "  label: ClassLabel(names=['negative', 'positive'], id=None)\n",
      "\n",
      "Label distribution:\n",
      "  train: {0: 29780, 1: 37569}\n",
      "  validation: {1: 444, 0: 428}\n",
      "  test: {-1: 1821}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset info:\")\n",
    "print(f\"Dataset type: {type(dataset)}\")\n",
    "print(f\"Dataset splits: {dataset.keys()}\")\n",
    "print(\"\\nNumber of examples per split:\")\n",
    "for split in dataset.keys():\n",
    "  print(f\"  {split}: {len(dataset[split])}\")\n",
    "\n",
    "# Examine features in the dataset\n",
    "print(\"\\nFeatures in the dataset:\")\n",
    "for feature in dataset['train'].features:\n",
    "  print(f\"  {feature}: {dataset['train'].features[feature]}\")\n",
    "\n",
    "# Look at label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "for split in dataset.keys():\n",
    "  label_counts = {}\n",
    "  for label in dataset[split]['label']:\n",
    "    if label not in label_counts:\n",
    "      label_counts[label] = 0\n",
    "    label_counts[label] += 1\n",
    "  print(f\"  {split}: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cc05f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training positive samples: 37569\n",
      "Original training negative samples: 29780\n",
      "Total samples in balanced training set: 59560\n",
      "Validation positive samples: 444\n",
      "Validation negative samples: 428\n",
      "Total samples in balanced validation set: 856\n",
      "\n",
      "Total samples in combined dataset: 60416\n",
      "Combined dataset label distribution: {1: 30208, 0: 30208}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Step 1: Balance the training set\n",
    "# Split dataset into positive and negative samples for the training set\n",
    "train_positive = dataset['train'].filter(lambda example: example['label'] == 1)\n",
    "train_negative = dataset['train'].filter(lambda example: example['label'] == 0)\n",
    "\n",
    "print(f\"Original training positive samples: {len(train_positive)}\")\n",
    "print(f\"Original training negative samples: {len(train_negative)}\")\n",
    "\n",
    "# Downsample the positive class to match negative class size\n",
    "positive_indices = np.random.choice(len(train_positive), len(train_negative), replace=False)\n",
    "downsampled_train_positive = train_positive.select(positive_indices)\n",
    "\n",
    "# Combine the balanced datasets for training\n",
    "balanced_train = concatenate_datasets([downsampled_train_positive, train_negative])\n",
    "balanced_train = balanced_train.shuffle(seed=config[\"seed\"])\n",
    "\n",
    "print(f\"Total samples in balanced training set: {len(balanced_train)}\")\n",
    "\n",
    "# Step 2: Balance the validation set\n",
    "val_positive = dataset['validation'].filter(lambda example: example['label'] == 1)\n",
    "val_negative = dataset['validation'].filter(lambda example: example['label'] == 0)\n",
    "\n",
    "print(f\"Validation positive samples: {len(val_positive)}\")\n",
    "print(f\"Validation negative samples: {len(val_negative)}\")\n",
    "\n",
    "# Balance validation set\n",
    "if len(val_positive) > len(val_negative):\n",
    "    val_positive_indices = np.random.choice(len(val_positive), len(val_negative), replace=False)\n",
    "    balanced_val_positive = val_positive.select(val_positive_indices)\n",
    "    balanced_validation = concatenate_datasets([balanced_val_positive, val_negative])\n",
    "else:\n",
    "    val_negative_indices = np.random.choice(len(val_negative), len(val_positive), replace=False)\n",
    "    balanced_val_negative = val_negative.select(val_negative_indices)\n",
    "    balanced_validation = concatenate_datasets([val_positive, balanced_val_negative])\n",
    "\n",
    "balanced_validation = balanced_validation.shuffle(seed=config[\"seed\"])\n",
    "print(f\"Total samples in balanced validation set: {len(balanced_validation)}\")\n",
    "\n",
    "# Step 3: Combine balanced train and validation into a single dataset\n",
    "combined_dataset = concatenate_datasets([balanced_train, balanced_validation])\n",
    "combined_dataset = combined_dataset.shuffle(seed=config[\"seed\"])\n",
    "\n",
    "print(f\"\\nTotal samples in combined dataset: {len(combined_dataset)}\")\n",
    "\n",
    "# Check the final class distribution\n",
    "combined_balance = {}\n",
    "for label in combined_dataset['label']:\n",
    "    if label not in combined_balance:\n",
    "        combined_balance[label] = 0\n",
    "    combined_balance[label] += 1\n",
    "print(f\"Combined dataset label distribution: {combined_balance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ac627d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text length statistics (characters):\n",
      "  Min: 2\n",
      "  Max: 268\n",
      "  Mean: 54.43\n",
      "  Median: 40.00\n",
      "\n",
      "Word count statistics:\n",
      "  Min: 1\n",
      "  Max: 52\n",
      "  Mean: 9.60\n",
      "  Median: 7.00\n",
      "\n",
      "Sample examples from combined dataset:\n",
      "  Example 1:\n",
      "    Text: the emotion is impressively true for being so hot-blooded \n",
      "    Label: 1 (Positive)\n",
      "\n",
      "  Example 2:\n",
      "    Text: botches \n",
      "    Label: 0 (Negative)\n",
      "\n",
      "  Example 3:\n",
      "    Text: tricky and satisfying as any of david \n",
      "    Label: 1 (Positive)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate text length statistics\n",
    "print(\"\\nText length statistics (characters):\")\n",
    "lengths = [len(text) for text in combined_dataset['sentence']]\n",
    "print(f\"  Min: {min(lengths)}\")\n",
    "print(f\"  Max: {max(lengths)}\")\n",
    "print(f\"  Mean: {np.mean(lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(lengths):.2f}\")\n",
    "\n",
    "# Show word count statistics\n",
    "print(\"\\nWord count statistics:\")\n",
    "word_counts = [len(text.split()) for text in combined_dataset['sentence']]\n",
    "print(f\"  Min: {min(word_counts)}\")\n",
    "print(f\"  Max: {max(word_counts)}\")\n",
    "print(f\"  Mean: {np.mean(word_counts):.2f}\")\n",
    "print(f\"  Median: {np.median(word_counts):.2f}\")\n",
    "\n",
    "# Show some examples from the combined dataset\n",
    "print(\"\\nSample examples from combined dataset:\")\n",
    "for i in range(3):  # Show 3 examples\n",
    "    print(f\"  Example {i+1}:\")\n",
    "    print(f\"    Text: {combined_dataset['sentence'][i]}\")\n",
    "    print(f\"    Label: {combined_dataset['label'][i]} ({['Negative', 'Positive'][combined_dataset['label'][i]]})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c29d095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: Train=42291, Val=9062, Dev=6041, Test=3022\n",
      "\n",
      "Final dataset sizes:\n",
      "Train: 42291 samples (70.0%)\n",
      "Validation: 9062 samples (15.0%)\n",
      "Dev: 6041 samples (10.0%)\n",
      "Test: 3022 samples (5.0%)\n",
      "Train label distribution: {1: 21226, 0: 21065}\n",
      "Validation label distribution: {1: 4508, 0: 4554}\n",
      "Dev label distribution: {0: 3107, 1: 2934}\n",
      "Test label distribution: {1: 1540, 0: 1482}\n"
     ]
    }
   ],
   "source": [
    "# Split the combined dataset\n",
    "dataset_size = len(combined_dataset)\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(dataset_size * config[\"train_split_ratio\"])\n",
    "val_size = int(dataset_size * config[\"validation_split_ratio\"])\n",
    "dev_size = int(dataset_size * config[\"dev_split_ratio\"])\n",
    "# The test_size will be the remainder\n",
    "test_size = dataset_size - train_size - val_size - dev_size\n",
    "\n",
    "print(f\"Split sizes: Train={train_size}, Val={val_size}, Dev={dev_size}, Test={test_size}\")\n",
    "\n",
    "# Create splits - combined_dataset is already shuffled with seed=42\n",
    "train_dataset = combined_dataset.select(range(train_size))\n",
    "val_dataset = combined_dataset.select(range(train_size, train_size + val_size))\n",
    "dev_dataset = combined_dataset.select(range(train_size + val_size, train_size + val_size + dev_size))\n",
    "test_dataset = combined_dataset.select(range(train_size + val_size + dev_size, dataset_size))\n",
    "\n",
    "# Verify sizes\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset)} samples ({len(train_dataset)/dataset_size*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_dataset)} samples ({len(val_dataset)/dataset_size*100:.1f}%)\")\n",
    "print(f\"Dev: {len(dev_dataset)} samples ({len(dev_dataset)/dataset_size*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_dataset)} samples ({len(test_dataset)/dataset_size*100:.1f}%)\")\n",
    "\n",
    "# Check label distribution in each split\n",
    "for split_name, split_dataset in [(\"Train\", train_dataset), (\"Validation\", val_dataset), \n",
    "                                 (\"Dev\", dev_dataset), (\"Test\", test_dataset)]:\n",
    "    label_counts = {}\n",
    "    for label in split_dataset['label']:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "    print(f\"{split_name} label distribution: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a7db0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], \n",
    "                    padding=config[\"tokenizer_padding\"], \n",
    "                    truncation=config[\"tokenizer_truncation\"],\n",
    "                    max_length=config[\"tokenizer_max_length\"])\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, num_proc=config[\"num_proc\"])\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, num_proc=config[\"num_proc\"])\n",
    "tokenized_dev = dev_dataset.map(tokenize_function, batched=True, num_proc=config[\"num_proc\"])\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, num_proc=config[\"num_proc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11bbd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65a52e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    per_device_train_batch_size=config[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"eval_batch_size\"],\n",
    "    num_train_epochs=config[\"num_epochs\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    "    eval_strategy=config[\"evaluation_strategy\"],\n",
    "    use_mps_device=config[\"use_mps_device\"],\n",
    "    seed=config[\"seed\"],\n",
    "    data_seed=config[\"seed\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29d172f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train.select(range(config[\"train_subset_size\"])),\n",
    "    eval_dataset=tokenized_val.select(range(config[\"validation_subset_size\"])),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4da1c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 01:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.714729</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704896</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.630137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=26, training_loss=0.6835442322951096, metrics={'train_runtime': 90.9511, 'train_samples_per_second': 4.398, 'train_steps_per_second': 0.286, 'total_flos': 52986959462400.0, 'train_loss': 0.6835442322951096, 'epoch': 2.0})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d184477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "  eval_loss: 0.7049\n",
      "  eval_accuracy: 0.4600\n",
      "  eval_precision: 0.4600\n",
      "  eval_recall: 1.0000\n",
      "  eval_f1: 0.6301\n",
      "  eval_runtime: 3.1625\n",
      "  eval_samples_per_second: 15.8100\n",
      "  eval_steps_per_second: 1.2650\n",
      "  epoch: 2.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation set\n",
    "val_results = trainer.evaluate()\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in val_results.items():\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ddb0ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev set evaluation results:\n",
      "  eval_loss: 0.7017\n",
      "  eval_model_preparation_time: 0.0009\n",
      "  eval_accuracy: 0.4200\n",
      "  eval_precision: 0.4200\n",
      "  eval_recall: 1.0000\n",
      "  eval_f1: 0.5915\n",
      "  eval_runtime: 3.1590\n",
      "  eval_samples_per_second: 15.8280\n",
      "  eval_steps_per_second: 1.2660\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the dev set\n",
    "dev_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_dev.select(range(config[\"dev_subset_size\"])),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "dev_results = dev_trainer.evaluate()\n",
    "print(\"\\nDev set evaluation results:\")\n",
    "for key, value in dev_results.items():\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "95f8c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set evaluation results:\n",
      "  eval_loss: 0.6580\n",
      "  eval_model_preparation_time: 0.0012\n",
      "  eval_accuracy: 0.6200\n",
      "  eval_precision: 0.6200\n",
      "  eval_recall: 1.0000\n",
      "  eval_f1: 0.7654\n",
      "  eval_runtime: 3.6971\n",
      "  eval_samples_per_second: 13.5240\n",
      "  eval_steps_per_second: 1.0820\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_test.select(range(config[\"test_subset_size\"]))\n",
    ")\n",
    "\n",
    "test_results = test_trainer.evaluate()\n",
    "print(\"\\nTest set evaluation results:\")\n",
    "for key, value in test_results.items():\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "250341a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base (non-fine-tuned) model test results:\n",
      "  eval_loss: 0.6950\n",
      "  eval_model_preparation_time: 0.0007\n",
      "  eval_accuracy: 0.4200\n",
      "  eval_precision: 0.6667\n",
      "  eval_recall: 0.1290\n",
      "  eval_f1: 0.2162\n",
      "  eval_runtime: 3.9650\n",
      "  eval_samples_per_second: 12.6100\n",
      "  eval_steps_per_second: 1.0090\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the base model on test dataset\n",
    "base_test_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_test.select(range(config[\"test_subset_size\"]))\n",
    ")\n",
    "\n",
    "base_test_results = base_test_trainer.evaluate()\n",
    "print(\"\\nBase (non-fine-tuned) model test results:\")\n",
    "for key, value in base_test_results.items():\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05ad864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SVM baseline model...\n",
      "Training SVM on 200 examples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
       "                (&#x27;svm&#x27;, LinearSVC(dual=False, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
       "                (&#x27;svm&#x27;, LinearSVC(dual=False, random_state=42))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(max_features=10000, ngram_range=(1, 2))</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearSVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC(dual=False, random_state=42)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
       "                ('svm', LinearSVC(dual=False, random_state=42))])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add SVM baseline for sentiment analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Create TF-IDF + SVM pipeline\n",
    "print(\"Creating SVM baseline model...\")\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "    ('svm', LinearSVC(random_state=config[\"seed\"], dual=False))\n",
    "])\n",
    "\n",
    "# Train SVM on the same training data subset as DistilBERT for fair comparison\n",
    "X_train_subset = [train_dataset['sentence'][i] for i in range(config[\"train_subset_size\"])]\n",
    "y_train_subset = [train_dataset['label'][i] for i in range(config[\"train_subset_size\"])]\n",
    "\n",
    "print(f\"Training SVM on {len(X_train_subset)} examples...\")\n",
    "svm_pipeline.fit(X_train_subset, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e4431c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM model test results:\n",
      "  accuracy: 0.6000\n",
      "  precision: 0.6571\n",
      "  recall: 0.7419\n",
      "  f1: 0.6970\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the same test subset as DistilBERT (50 examples)\n",
    "X_test_subset = [test_dataset['sentence'][i] for i in range(config[\"test_subset_size\"])]\n",
    "y_test_subset = [test_dataset['label'][i] for i in range(config[\"test_subset_size\"])]\n",
    "\n",
    "svm_predictions = svm_pipeline.predict(X_test_subset)\n",
    "\n",
    "# Calculate metrics using the same function used for transformers\n",
    "svm_accuracy = accuracy_score(y_test_subset, svm_predictions)\n",
    "svm_precision, svm_recall, svm_f1, _ = precision_recall_fscore_support(\n",
    "    y_test_subset, svm_predictions, average='binary'\n",
    ")\n",
    "\n",
    "# Create a dictionary of SVM results similar to the transformer model results\n",
    "svm_test_results = {\n",
    "    'eval_accuracy': svm_accuracy,\n",
    "    'eval_precision': svm_precision,\n",
    "    'eval_recall': svm_recall,\n",
    "    'eval_f1': svm_f1,\n",
    "    'accuracy': svm_accuracy,\n",
    "    'precision': svm_precision, \n",
    "    'recall': svm_recall,\n",
    "    'f1': svm_f1\n",
    "}\n",
    "\n",
    "# Display SVM metrics in the same format as the other models\n",
    "print(\"\\nSVM model test results:\")\n",
    "for key, value in svm_test_results.items():\n",
    "  if key.startswith('eval_'):  # Skip duplicate metrics with 'eval_' prefix\n",
    "    continue\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f7ae92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance comparison (Test Set):\n",
      "Metric                         Fine-tuned      Base model      SVM            \n",
      "---------------------------------------------------------------------------\n",
      "eval_loss                      0.6580         0.6950         0.0000\n",
      "eval_model_preparation_time    0.0012         0.0007         0.0000\n",
      "eval_accuracy                  0.6200         0.4200         0.6000\n",
      "eval_precision                 0.6200         0.6667         0.6571\n",
      "eval_recall                    1.0000         0.1290         0.7419\n",
      "eval_f1                        0.7654         0.2162         0.6970\n",
      "eval_runtime                   3.6971         3.9650         0.0000\n",
      "eval_samples_per_second        13.5240         12.6100         0.0000\n",
      "eval_steps_per_second          1.0820         1.0090         0.0000\n"
     ]
    }
   ],
   "source": [
    "# Enhanced comparison including the SVM model\n",
    "print(\"\\nPerformance comparison (Test Set):\")\n",
    "print(f\"{'Metric':<30} {'Fine-tuned':<15} {'Base model':<15} {'SVM':<15}\")\n",
    "print(\"-\" * 75)\n",
    "for key in test_results:\n",
    "    if isinstance(test_results[key], float):\n",
    "        # Use svm_test_results dictionary directly\n",
    "        svm_value = svm_test_results.get(key, 0)\n",
    "        improvement_ft = test_results[key] - base_test_results[key]\n",
    "        improvement_svm = svm_value - base_test_results[key]\n",
    "        print(f\"{key:<30} {test_results[key]:.4f}{'':<8} {base_test_results[key]:.4f}{'':<8} {svm_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5aabb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to determine the appropriate device\n",
    "def get_device():\n",
    "    \"\"\"Determine whether to use MPS or CPU based on availability.\"\"\"\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "# Function to perform sentiment prediction using the fine-tuned model\n",
    "def predict_sentiment(text):\n",
    "    # Try using the preferred device first\n",
    "    device = get_device()\n",
    "    \n",
    "    try:\n",
    "        # Create inputs and move to appropriate device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", \n",
    "                      padding=config[\"tokenizer_padding\"], \n",
    "                      truncation=config[\"tokenizer_truncation\"], \n",
    "                      max_length=config[\"tokenizer_max_length\"])\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform prediction\n",
    "        return perform_prediction(inputs, device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If there's an error with the preferred device, fall back to CPU\n",
    "        if device.type != \"cpu\":\n",
    "            print(f\"Error with {device.type}: {e}. Falling back to CPU.\")\n",
    "            device = torch.device(\"cpu\")\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", \n",
    "                      padding=config[\"tokenizer_padding\"], \n",
    "                      truncation=config[\"tokenizer_truncation\"], \n",
    "                      max_length=config[\"tokenizer_max_length\"])\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            return perform_prediction(inputs, device)\n",
    "        else:\n",
    "            # If we're already on CPU and still getting an error, raise it\n",
    "            raise e\n",
    "\n",
    "# Helper function to perform the actual prediction\n",
    "def perform_prediction(inputs, device):\n",
    "    \"\"\"Perform sentiment prediction with the model on the specified device.\"\"\"\n",
    "    # Move model to device for inference\n",
    "    model_on_device = model.to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model_on_device(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "    if predictions == 1:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7e238545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment prediction using the base (non-fine-tuned) model\n",
    "def predict_sentiment_base(text):\n",
    "    device = get_device()\n",
    "    \n",
    "    try:\n",
    "        inputs = base_tokenizer(text, return_tensors=\"pt\", \n",
    "                           padding=config[\"tokenizer_padding\"], \n",
    "                           truncation=config[\"tokenizer_truncation\"], \n",
    "                           max_length=config[\"tokenizer_max_length\"])\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Move base model to device for inference\n",
    "        base_model_on_device = base_model.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = base_model_on_device(**inputs)\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "        if predictions == 1:\n",
    "            return \"Positive\"\n",
    "        else:\n",
    "            return \"Negative\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        if device.type != \"cpu\":\n",
    "            print(f\"Error with {device.type}: {e}. Falling back to CPU.\")\n",
    "            device = torch.device(\"cpu\")\n",
    "            inputs = base_tokenizer(text, return_tensors=\"pt\", \n",
    "                           padding=config[\"tokenizer_padding\"], \n",
    "                           truncation=config[\"tokenizer_truncation\"], \n",
    "                           max_length=config[\"tokenizer_max_length\"])\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Retry with CPU\n",
    "            base_model_on_device = base_model.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = base_model_on_device(**inputs)\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1).item()\n",
    "            return \"Positive\" if predictions == 1 else \"Negative\"\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36c65c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions with SVM\n",
    "def predict_sentiment_svm(text):\n",
    "    prediction = svm_pipeline.predict([text])[0]\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cfb0f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing predictions across all models:\n",
      "Text                                                         Ground Truth    Fine-tuned      Base model      SVM            \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Bad                                                          Negative        Positive        Positive        Negative       \n",
      "Good                                                         Positive        Positive        Positive        Positive       \n",
      "I hate this                                                  Negative        Positive        Positive        Positive       \n",
      "I love this                                                  Positive        Positive        Negative        Positive       \n",
      "This movie was OK                                            Positive        Positive        Negative        Positive       \n",
      "This movie was fantastic                                     Positive        Positive        Negative        Positive       \n",
      "This movie was terrible                                      Negative        Positive        Negative        Positive       \n",
      "This is not bad                                              Positive        Positive        Negative        Negative       \n",
      "Good movie but bad acting                                    Positive        Positive        Negative        Negative       \n",
      "Despite the poor beginning, the ending was great             Positive        Positive        Negative        Positive       \n",
      "The plot was intricate and the characters were well develope... Positive        Positive        Negative        Positive       \n",
      "A masterpiece of modern cinema with stunning visuals         Positive        Positive        Negative        Positive       \n",
      "The director failed to engage the audience                   Negative        Positive        Negative        Negative       \n",
      "Not the best film I've seen, but still enjoyable             Positive        Positive        Negative        Negative       \n",
      "I wouldn't recommend this to anyone                          Negative        Positive        Positive        Positive       \n",
      "It wasn't as bad as the critics suggested                    Positive        Positive        Negative        Negative       \n",
      "Absolutely brilliant performances by the entire cast         Positive        Positive        Negative        Positive       \n",
      "A complete waste of time and money                           Negative        Positive        Negative        Positive       \n",
      "The special effects couldn't save the weak storyline         Negative        Positive        Negative        Negative       \n",
      "Despite its flaws, the film manages to be entertaining       Positive        Positive        Negative        Positive       \n",
      "It's so bad it's actually good                               Positive        Positive        Negative        Negative       \n",
      "The film offers nothing new to the genre                     Negative        Positive        Negative        Positive       \n",
      "While not perfect, it exceeded my expectations               Positive        Positive        Negative        Negative       \n",
      "The soundtrack was the only redeeming quality                Negative        Positive        Negative        Negative       \n",
      "\n",
      "Model agreement:\n",
      "Fine-tuned/Base agreement: 4/24 (16.7%)\n",
      "Fine-tuned/SVM agreement: 14/24 (58.3%)\n",
      "Base/SVM agreement: 12/24 (50.0%)\n",
      "\n",
      "Model accuracy on test examples:\n",
      "Fine-tuned model: 15/24 (62.5%)\n",
      "Base model: 7/24 (29.2%)\n",
      "SVM model: 13/24 (54.2%)\n",
      "Best performing model: Fine-tuned\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions between fine-tuned, base models, and SVM\n",
    "test_examples = [\n",
    "    (\"Bad\", \"Negative\"),\n",
    "    (\"Good\", \"Positive\"),\n",
    "    (\"I hate this\", \"Negative\"),\n",
    "    (\"I love this\", \"Positive\"),\n",
    "    (\"This movie was OK\", \"Positive\"),\n",
    "    (\"This movie was fantastic\", \"Positive\"),\n",
    "    (\"This movie was terrible\", \"Negative\"),\n",
    "    (\"This is not bad\", \"Positive\"),\n",
    "    (\"Good movie but bad acting\", \"Positive\"),\n",
    "    (\"Despite the poor beginning, the ending was great\", \"Positive\"),\n",
    "    (\"The plot was intricate and the characters were well developed\", \"Positive\"),\n",
    "    (\"A masterpiece of modern cinema with stunning visuals\", \"Positive\"),    \n",
    "    (\"The director failed to engage the audience\", \"Negative\"),\n",
    "    (\"Not the best film I've seen, but still enjoyable\", \"Positive\"),\n",
    "    (\"I wouldn't recommend this to anyone\", \"Negative\"),\n",
    "    (\"It wasn't as bad as the critics suggested\", \"Positive\"),\n",
    "    (\"Absolutely brilliant performances by the entire cast\", \"Positive\"),\n",
    "    (\"A complete waste of time and money\", \"Negative\"),\n",
    "    (\"The special effects couldn't save the weak storyline\", \"Negative\"),\n",
    "    (\"Despite its flaws, the film manages to be entertaining\", \"Positive\"),\n",
    "    (\"It's so bad it's actually good\", \"Positive\"),\n",
    "    (\"The film offers nothing new to the genre\", \"Negative\"),\n",
    "    (\"While not perfect, it exceeded my expectations\", \"Positive\"),\n",
    "    (\"The soundtrack was the only redeeming quality\", \"Negative\")\n",
    "]\n",
    "\n",
    "print(\"\\nComparing predictions across all models:\")\n",
    "print(f\"{'Text':<60} {'Ground Truth':<15} {'Fine-tuned':<15} {'Base model':<15} {'SVM':<15}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "matches_ft_base = 0\n",
    "matches_ft_svm = 0\n",
    "matches_base_svm = 0\n",
    "fine_tuned_correct = 0\n",
    "base_correct = 0\n",
    "svm_correct = 0\n",
    "\n",
    "for example, ground_truth in test_examples:\n",
    "    fine_tuned_pred = predict_sentiment(example)\n",
    "    base_pred = predict_sentiment_base(example)\n",
    "    svm_pred = predict_sentiment_svm(example)\n",
    "    \n",
    "    # Count matches between models\n",
    "    if fine_tuned_pred == base_pred:\n",
    "        matches_ft_base += 1\n",
    "    if fine_tuned_pred == svm_pred:\n",
    "        matches_ft_svm += 1\n",
    "    if base_pred == svm_pred:\n",
    "        matches_base_svm += 1\n",
    "    \n",
    "    # Count correct predictions\n",
    "    if ground_truth in [\"Positive\", \"Negative\"]:\n",
    "        if fine_tuned_pred == ground_truth:\n",
    "            fine_tuned_correct += 1\n",
    "        if base_pred == ground_truth:\n",
    "            base_correct += 1\n",
    "        if svm_pred == ground_truth:\n",
    "            svm_correct += 1\n",
    "    \n",
    "    # Truncate long examples for display\n",
    "    display_text = example[:60] + \"...\" if len(example) > 60 else example\n",
    "    print(f\"{display_text:<60} {ground_truth:<15} {fine_tuned_pred:<15} {base_pred:<15} {svm_pred:<15}\")\n",
    "\n",
    "# Calculate metrics\n",
    "binary_examples = sum(1 for _, label in test_examples if label in [\"Positive\", \"Negative\"])\n",
    "fine_tuned_acc = (fine_tuned_correct / binary_examples) * 100 if binary_examples > 0 else 0\n",
    "base_acc = (base_correct / binary_examples) * 100 if binary_examples > 0 else 0\n",
    "svm_acc = (svm_correct / binary_examples) * 100 if binary_examples > 0 else 0\n",
    "\n",
    "# Calculate agreement percentages\n",
    "ft_base_agreement = (matches_ft_base / len(test_examples)) * 100\n",
    "ft_svm_agreement = (matches_ft_svm / len(test_examples)) * 100\n",
    "base_svm_agreement = (matches_base_svm / len(test_examples)) * 100\n",
    "\n",
    "# Determine which model is best\n",
    "models = [\n",
    "    (\"Fine-tuned\", fine_tuned_acc),\n",
    "    (\"Base\", base_acc),\n",
    "    (\"SVM\", svm_acc)\n",
    "]\n",
    "best_model = max(models, key=lambda x: x[1])[0]\n",
    "\n",
    "print(f\"\\nModel agreement:\")\n",
    "print(f\"Fine-tuned/Base agreement: {matches_ft_base}/{len(test_examples)} ({ft_base_agreement:.1f}%)\")\n",
    "print(f\"Fine-tuned/SVM agreement: {matches_ft_svm}/{len(test_examples)} ({ft_svm_agreement:.1f}%)\")\n",
    "print(f\"Base/SVM agreement: {matches_base_svm}/{len(test_examples)} ({base_svm_agreement:.1f}%)\")\n",
    "\n",
    "print(f\"\\nModel accuracy on test examples:\")\n",
    "print(f\"Fine-tuned model: {fine_tuned_correct}/{binary_examples} ({fine_tuned_acc:.1f}%)\")\n",
    "print(f\"Base model: {base_correct}/{binary_examples} ({base_acc:.1f}%)\")\n",
    "print(f\"SVM model: {svm_correct}/{binary_examples} ({svm_acc:.1f}%)\")\n",
    "print(f\"Best performing model: {best_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs681-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
