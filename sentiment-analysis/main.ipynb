{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85142507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the DistilBERT model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bf48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Yelp Polarity dataset\n",
    "dataset = load_dataset('yelp_polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a81d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: dict_keys(['train', 'test'])\n",
      "Train set size: 560000\n",
      "Test set size: 38000\n",
      "\n",
      "Sample example from train set:\n",
      "{'text': \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\", 'label': 0}\n",
      "\n",
      "Label distribution in train set:\n",
      "Label 0 (negative): 280000\n",
      "Label 1 (positive): 280000\n",
      "\n",
      "Label distribution in test set:\n",
      "Label 0 (negative): 19000\n",
      "Label 1 (positive): 19000\n",
      "\n",
      "Negative review example:\n",
      "Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply ...\n",
      "\n",
      "Positive review example:\n",
      "Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of hi...\n"
     ]
    }
   ],
   "source": [
    "# Explore dataset\n",
    "# Check dataset structure\n",
    "print(\"Dataset structure:\", dataset.keys())\n",
    "\n",
    "# Check size of train and test sets\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")\n",
    "\n",
    "# Examine data format\n",
    "print(\"\\nSample example from train set:\")\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# Check label distribution\n",
    "train_labels = [example['label'] for example in dataset['train']]\n",
    "test_labels = [example['label'] for example in dataset['test']]\n",
    "\n",
    "print(\"\\nLabel distribution in train set:\")\n",
    "print(f\"Label 0 (negative): {train_labels.count(0)}\")\n",
    "print(f\"Label 1 (positive): {train_labels.count(1)}\")\n",
    "\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(f\"Label 0 (negative): {test_labels.count(0)}\")\n",
    "print(f\"Label 1 (positive): {test_labels.count(1)}\")\n",
    "\n",
    "# Display some examples\n",
    "print(\"\\nNegative review example:\")\n",
    "neg_idx = train_labels.index(0)\n",
    "print(dataset['train'][neg_idx]['text'][:200] + \"...\")\n",
    "\n",
    "print(\"\\nPositive review example:\")\n",
    "pos_idx = train_labels.index(1)\n",
    "print(dataset['train'][pos_idx]['text'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7db0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_train = dataset['train'].map(tokenize_function, batched=True, num_proc=4)\n",
    "tokenized_test = dataset['test'].map(tokenize_function, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a52e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/transformers/training_args.py:2243: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d172f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train.select(range(1000)),  # Use a subset for faster training\n",
    "    eval_dataset=tokenized_test.select(range(500)),     # Use a subset for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da1c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 03:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.253004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=0.26669405377100386, metrics={'train_runtime': 207.7138, 'train_samples_per_second': 4.814, 'train_steps_per_second': 0.303, 'total_flos': 132467398656000.0, 'train_loss': 0.26669405377100386, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d184477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.25300395488739014,\n",
       " 'eval_runtime': 29.5554,\n",
       " 'eval_samples_per_second': 16.917,\n",
       " 'eval_steps_per_second': 1.083,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38895f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_yelp_model/tokenizer_config.json',\n",
       " './fine_tuned_yelp_model/special_tokens_map.json',\n",
       " './fine_tuned_yelp_model/vocab.txt',\n",
       " './fine_tuned_yelp_model/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model for future use\n",
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5aabb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to determine the appropriate device\n",
    "def get_device():\n",
    "    \"\"\"Determine whether to use MPS or CPU based on availability.\"\"\"\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS acceleration\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"MPS not available, using CPU\")\n",
    "    return device\n",
    "\n",
    "# Function to perform sentiment prediction using the fine-tuned model\n",
    "def predict_sentiment(text):\n",
    "    # Try using the preferred device first\n",
    "    device = get_device()\n",
    "    \n",
    "    try:\n",
    "        # Create inputs and move to appropriate device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform prediction\n",
    "        return perform_prediction(inputs, device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If there's an error with the preferred device, fall back to CPU\n",
    "        if device.type != \"cpu\":\n",
    "            print(f\"Error with {device.type}: {e}. Falling back to CPU.\")\n",
    "            device = torch.device(\"cpu\")\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            return perform_prediction(inputs, device)\n",
    "        else:\n",
    "            # If we're already on CPU and still getting an error, raise it\n",
    "            raise e\n",
    "\n",
    "# Helper function to perform the actual prediction\n",
    "def perform_prediction(inputs, device):\n",
    "    \"\"\"Perform sentiment prediction with the model on the specified device.\"\"\"\n",
    "    # Move model to device for inference\n",
    "    model_on_device = model.to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model_on_device(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "    if predictions == 1:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51da64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS acceleration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(text=\"I hate this\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs681-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
