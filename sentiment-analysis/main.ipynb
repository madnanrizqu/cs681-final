{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "96364735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All random seeds have been set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from transformers import set_seed\n",
    "\n",
    "# Define a single seed value to use throughout the code\n",
    "SEED = 42\n",
    "\n",
    "# Set seeds for Python's random module\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seed for Hugging Face Transformers\n",
    "set_seed(SEED)\n",
    "\n",
    "# For some operations on Apple Silicon (MPS)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "print(f\"All random seeds have been set to {SEED} for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "85142507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the DistilBERT model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c089f527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh instance of the base DistilBERT model\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the base model and tokenizer (will not use your fine-tuned weights)\n",
    "base_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "44bf48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3a81d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "Dataset type: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "Number of examples per split:\n",
      "  train: 67349\n",
      "  validation: 872\n",
      "  test: 1821\n",
      "\n",
      "Features in the dataset:\n",
      "  idx: Value(dtype='int32', id=None)\n",
      "  sentence: Value(dtype='string', id=None)\n",
      "  label: ClassLabel(names=['negative', 'positive'], id=None)\n",
      "\n",
      "Label distribution:\n",
      "  train: {0: 29780, 1: 37569}\n",
      "  validation: {1: 444, 0: 428}\n",
      "  test: {-1: 1821}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset info:\")\n",
    "print(f\"Dataset type: {type(dataset)}\")\n",
    "print(f\"Dataset splits: {dataset.keys()}\")\n",
    "print(\"\\nNumber of examples per split:\")\n",
    "for split in dataset.keys():\n",
    "  print(f\"  {split}: {len(dataset[split])}\")\n",
    "\n",
    "# Examine features in the dataset\n",
    "print(\"\\nFeatures in the dataset:\")\n",
    "for feature in dataset['train'].features:\n",
    "  print(f\"  {feature}: {dataset['train'].features[feature]}\")\n",
    "\n",
    "# Look at label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "for split in dataset.keys():\n",
    "  label_counts = {}\n",
    "  for label in dataset[split]['label']:\n",
    "    if label not in label_counts:\n",
    "      label_counts[label] = 0\n",
    "    label_counts[label] += 1\n",
    "  print(f\"  {split}: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4cc05f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training positive samples: 37569\n",
      "Original training negative samples: 29780\n",
      "Total samples in balanced training set: 59560\n",
      "Validation positive samples: 444\n",
      "Validation negative samples: 428\n",
      "Total samples in balanced validation set: 856\n",
      "\n",
      "Total samples in combined dataset: 60416\n",
      "Combined dataset label distribution: {1: 30208, 0: 30208}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Step 1: Balance the training set\n",
    "# Split dataset into positive and negative samples for the training set\n",
    "train_positive = dataset['train'].filter(lambda example: example['label'] == 1)\n",
    "train_negative = dataset['train'].filter(lambda example: example['label'] == 0)\n",
    "\n",
    "print(f\"Original training positive samples: {len(train_positive)}\")\n",
    "print(f\"Original training negative samples: {len(train_negative)}\")\n",
    "\n",
    "# Downsample the positive class to match negative class size\n",
    "np.random.seed(42)\n",
    "positive_indices = np.random.choice(len(train_positive), len(train_negative), replace=False)\n",
    "downsampled_train_positive = train_positive.select(positive_indices)\n",
    "\n",
    "# Combine the balanced datasets for training\n",
    "balanced_train = concatenate_datasets([downsampled_train_positive, train_negative])\n",
    "balanced_train = balanced_train.shuffle(seed=42)\n",
    "\n",
    "print(f\"Total samples in balanced training set: {len(balanced_train)}\")\n",
    "\n",
    "# Step 2: Balance the validation set\n",
    "val_positive = dataset['validation'].filter(lambda example: example['label'] == 1)\n",
    "val_negative = dataset['validation'].filter(lambda example: example['label'] == 0)\n",
    "\n",
    "print(f\"Validation positive samples: {len(val_positive)}\")\n",
    "print(f\"Validation negative samples: {len(val_negative)}\")\n",
    "\n",
    "# Balance validation set\n",
    "if len(val_positive) > len(val_negative):\n",
    "    val_positive_indices = np.random.choice(len(val_positive), len(val_negative), replace=False)\n",
    "    balanced_val_positive = val_positive.select(val_positive_indices)\n",
    "    balanced_validation = concatenate_datasets([balanced_val_positive, val_negative])\n",
    "else:\n",
    "    val_negative_indices = np.random.choice(len(val_negative), len(val_positive), replace=False)\n",
    "    balanced_val_negative = val_negative.select(val_negative_indices)\n",
    "    balanced_validation = concatenate_datasets([val_positive, balanced_val_negative])\n",
    "\n",
    "balanced_validation = balanced_validation.shuffle(seed=42)\n",
    "print(f\"Total samples in balanced validation set: {len(balanced_validation)}\")\n",
    "\n",
    "# Step 3: Combine balanced train and validation into a single dataset\n",
    "combined_dataset = concatenate_datasets([balanced_train, balanced_validation])\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"\\nTotal samples in combined dataset: {len(combined_dataset)}\")\n",
    "\n",
    "# Check the final class distribution\n",
    "combined_balance = {}\n",
    "for label in combined_dataset['label']:\n",
    "    if label not in combined_balance:\n",
    "        combined_balance[label] = 0\n",
    "    combined_balance[label] += 1\n",
    "print(f\"Combined dataset label distribution: {combined_balance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8ac627d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text length statistics (characters):\n",
      "  Min: 2\n",
      "  Max: 268\n",
      "  Mean: 54.43\n",
      "  Median: 40.00\n",
      "\n",
      "Word count statistics:\n",
      "  Min: 1\n",
      "  Max: 52\n",
      "  Mean: 9.60\n",
      "  Median: 7.00\n",
      "\n",
      "Sample examples from combined dataset:\n",
      "  Example 1:\n",
      "    Text: the emotion is impressively true for being so hot-blooded \n",
      "    Label: 1 (Positive)\n",
      "\n",
      "  Example 2:\n",
      "    Text: botches \n",
      "    Label: 0 (Negative)\n",
      "\n",
      "  Example 3:\n",
      "    Text: tricky and satisfying as any of david \n",
      "    Label: 1 (Positive)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate text length statistics\n",
    "print(\"\\nText length statistics (characters):\")\n",
    "lengths = [len(text) for text in combined_dataset['sentence']]\n",
    "print(f\"  Min: {min(lengths)}\")\n",
    "print(f\"  Max: {max(lengths)}\")\n",
    "print(f\"  Mean: {np.mean(lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(lengths):.2f}\")\n",
    "\n",
    "# Show word count statistics\n",
    "print(\"\\nWord count statistics:\")\n",
    "word_counts = [len(text.split()) for text in combined_dataset['sentence']]\n",
    "print(f\"  Min: {min(word_counts)}\")\n",
    "print(f\"  Max: {max(word_counts)}\")\n",
    "print(f\"  Mean: {np.mean(word_counts):.2f}\")\n",
    "print(f\"  Median: {np.median(word_counts):.2f}\")\n",
    "\n",
    "# Show some examples from the combined dataset\n",
    "print(\"\\nSample examples from combined dataset:\")\n",
    "for i in range(3):  # Show 3 examples\n",
    "    print(f\"  Example {i+1}:\")\n",
    "    print(f\"    Text: {combined_dataset['sentence'][i]}\")\n",
    "    print(f\"    Label: {combined_dataset['label'][i]} ({['Negative', 'Positive'][combined_dataset['label'][i]]})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1c29d095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: Train=42291, Val=9062, Dev=6041, Test=3022\n",
      "\n",
      "Final dataset sizes:\n",
      "Train: 42291 samples (70.0%)\n",
      "Validation: 9062 samples (15.0%)\n",
      "Dev: 6041 samples (10.0%)\n",
      "Test: 3022 samples (5.0%)\n",
      "Train label distribution: {1: 21226, 0: 21065}\n",
      "Validation label distribution: {1: 4508, 0: 4554}\n",
      "Dev label distribution: {0: 3107, 1: 2934}\n",
      "Test label distribution: {1: 1540, 0: 1482}\n"
     ]
    }
   ],
   "source": [
    "# Split the combined dataset into train (70%), validation (15%), dev (10%), and test (5%)\n",
    "dataset_size = len(combined_dataset)\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(dataset_size * 0.7)\n",
    "val_size = int(dataset_size * 0.15)\n",
    "dev_size = int(dataset_size * 0.1)\n",
    "# The test_size will be the remainder (approximately 5%)\n",
    "test_size = dataset_size - train_size - val_size - dev_size\n",
    "\n",
    "print(f\"Split sizes: Train={train_size}, Val={val_size}, Dev={dev_size}, Test={test_size}\")\n",
    "\n",
    "# Create splits - combined_dataset is already shuffled with seed=42\n",
    "train_dataset = combined_dataset.select(range(train_size))\n",
    "val_dataset = combined_dataset.select(range(train_size, train_size + val_size))\n",
    "dev_dataset = combined_dataset.select(range(train_size + val_size, train_size + val_size + dev_size))\n",
    "test_dataset = combined_dataset.select(range(train_size + val_size + dev_size, dataset_size))\n",
    "\n",
    "# Verify sizes\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset)} samples ({len(train_dataset)/dataset_size*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_dataset)} samples ({len(val_dataset)/dataset_size*100:.1f}%)\")\n",
    "print(f\"Dev: {len(dev_dataset)} samples ({len(dev_dataset)/dataset_size*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_dataset)} samples ({len(test_dataset)/dataset_size*100:.1f}%)\")\n",
    "\n",
    "# Check label distribution in each split\n",
    "for split_name, split_dataset in [(\"Train\", train_dataset), (\"Validation\", val_dataset), \n",
    "                                 (\"Dev\", dev_dataset), (\"Test\", test_dataset)]:\n",
    "    label_counts = {}\n",
    "    for label in split_dataset['label']:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "    print(f\"{split_name} label distribution: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a7db0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, num_proc=4)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, num_proc=4)\n",
    "tokenized_dev = dev_dataset.map(tokenize_function, batched=True, num_proc=4)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "11bbd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "65a52e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    use_mps_device=True,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "29d172f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train.select(range(200)),  # Use a subset for faster training\n",
    "    eval_dataset=tokenized_val.select(range(50)),     # Use a subset for evaluation\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4da1c48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 02:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.714729</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704896</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.630137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=26, training_loss=0.6835443056546725, metrics={'train_runtime': 159.2995, 'train_samples_per_second': 2.511, 'train_steps_per_second': 0.163, 'total_flos': 52986959462400.0, 'train_loss': 0.6835443056546725, 'epoch': 2.0})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d184477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "  eval_loss: 0.7049\n",
      "  eval_accuracy: 0.4600\n",
      "  eval_precision: 0.4600\n",
      "  eval_recall: 1.0000\n",
      "  eval_f1: 0.6301\n",
      "  eval_runtime: 3.6534\n",
      "  eval_samples_per_second: 13.6860\n",
      "  eval_steps_per_second: 1.0950\n",
      "  epoch: 2.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation set\n",
    "val_results = trainer.evaluate()\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in val_results.items():\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ddb0ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev set evaluation results:\n",
      "  eval_loss: 0.7017\n",
      "  eval_model_preparation_time: 0.0009\n",
      "  eval_accuracy: 0.4200\n",
      "  eval_precision: 0.4200\n",
      "  eval_recall: 1.0000\n",
      "  eval_f1: 0.5915\n",
      "  eval_runtime: 3.2796\n",
      "  eval_samples_per_second: 15.2460\n",
      "  eval_steps_per_second: 1.2200\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the dev set\n",
    "dev_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_dev.select(range(50)),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "dev_results = dev_trainer.evaluate()\n",
    "print(\"\\nDev set evaluation results:\")\n",
    "for key, value in dev_results.items():\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "95f8c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set evaluation results:\n",
      "  eval_loss: 0.6580\n",
      "  eval_model_preparation_time: 0.0006\n",
      "  eval_accuracy: 0.6200\n",
      "  eval_precision: 0.6200\n",
      "  eval_recall: 1.0000\n",
      "  eval_f1: 0.7654\n",
      "  eval_runtime: 3.3016\n",
      "  eval_samples_per_second: 15.1440\n",
      "  eval_steps_per_second: 1.2120\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_test.select(range(50))\n",
    ")\n",
    "\n",
    "test_results = test_trainer.evaluate()\n",
    "print(\"\\nTest set evaluation results:\")\n",
    "for key, value in test_results.items():\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "250341a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs681-final/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base (non-fine-tuned) model test results:\n",
      "  eval_loss: 0.6950\n",
      "  eval_model_preparation_time: 0.0006\n",
      "  eval_accuracy: 0.4200\n",
      "  eval_precision: 0.6667\n",
      "  eval_recall: 0.1290\n",
      "  eval_f1: 0.2162\n",
      "  eval_runtime: 3.7541\n",
      "  eval_samples_per_second: 13.3190\n",
      "  eval_steps_per_second: 1.0650\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the base model on test dataset\n",
    "base_test_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_test.select(range(50))\n",
    ")\n",
    "\n",
    "base_test_results = base_test_trainer.evaluate()\n",
    "print(\"\\nBase (non-fine-tuned) model test results:\")\n",
    "for key, value in base_test_results.items():\n",
    "  if isinstance(value, float):\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "  else:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f7ae92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance comparison (Test Set):\n",
      "Metric                         Fine-tuned      Base model      Difference     \n",
      "-----------------------------------------------------------------\n",
      "eval_loss                      0.6580         0.6950         -0.0370\n",
      "eval_model_preparation_time    0.0006         0.0006         0.0000\n",
      "eval_accuracy                  0.6200         0.4200         0.2000\n",
      "eval_precision                 0.6200         0.6667         -0.0467\n",
      "eval_recall                    1.0000         0.1290         0.8710\n",
      "eval_f1                        0.7654         0.2162         0.5492\n",
      "eval_runtime                   3.3016         3.7541         -0.4525\n",
      "eval_samples_per_second        15.1440         13.3190         1.8250\n",
      "eval_steps_per_second          1.2120         1.0650         0.1470\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show comparison with fine-tuned model\n",
    "print(\"\\nPerformance comparison (Test Set):\")\n",
    "print(f\"{'Metric':<30} {'Fine-tuned':<15} {'Base model':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for key in test_results:\n",
    "    if isinstance(test_results[key], float):\n",
    "        improvement = test_results[key] - base_test_results[key]\n",
    "        print(f\"{key:<30} {test_results[key]:.4f}{'':<8} {base_test_results[key]:.4f}{'':<8} {improvement:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "38895f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer_config.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/vocab.txt',\n",
       " './model/added_tokens.json')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model for future use\n",
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5aabb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to determine the appropriate device\n",
    "def get_device():\n",
    "    \"\"\"Determine whether to use MPS or CPU based on availability.\"\"\"\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "# Function to perform sentiment prediction using the fine-tuned model\n",
    "def predict_sentiment(text):\n",
    "    # Try using the preferred device first\n",
    "    device = get_device()\n",
    "    \n",
    "    try:\n",
    "        # Create inputs and move to appropriate device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform prediction\n",
    "        return perform_prediction(inputs, device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If there's an error with the preferred device, fall back to CPU\n",
    "        if device.type != \"cpu\":\n",
    "            print(f\"Error with {device.type}: {e}. Falling back to CPU.\")\n",
    "            device = torch.device(\"cpu\")\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            return perform_prediction(inputs, device)\n",
    "        else:\n",
    "            # If we're already on CPU and still getting an error, raise it\n",
    "            raise e\n",
    "\n",
    "# Helper function to perform the actual prediction\n",
    "def perform_prediction(inputs, device):\n",
    "    \"\"\"Perform sentiment prediction with the model on the specified device.\"\"\"\n",
    "    # Move model to device for inference\n",
    "    model_on_device = model.to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model_on_device(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "    if predictions == 1:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7e238545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment prediction using the base (non-fine-tuned) model\n",
    "def predict_sentiment_base(text):\n",
    "    device = get_device()\n",
    "    \n",
    "    try:\n",
    "        inputs = base_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Move base model to device for inference\n",
    "        base_model_on_device = base_model.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = base_model_on_device(**inputs)\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "        if predictions == 1:\n",
    "            return \"Positive\"\n",
    "        else:\n",
    "            return \"Negative\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        if device.type != \"cpu\":\n",
    "            print(f\"Error with {device.type}: {e}. Falling back to CPU.\")\n",
    "            device = torch.device(\"cpu\")\n",
    "            inputs = base_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Retry with CPU\n",
    "            base_model_on_device = base_model.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = base_model_on_device(**inputs)\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1).item()\n",
    "            return \"Positive\" if predictions == 1 else \"Negative\"\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cfb0f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing predictions between fine-tuned and base models:\n",
      "Text                                                         Ground Truth    Fine-tuned      Base model      Match?    \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Bad                                                          Negative        Positive        Positive        ✓         \n",
      "Good                                                         Positive        Positive        Positive        ✓         \n",
      "I hate this                                                  Negative        Positive        Positive        ✓         \n",
      "I love this                                                  Positive        Positive        Negative        ✗         \n",
      "This movie was OK                                            Positive        Positive        Negative        ✗         \n",
      "This movie was fantastic                                     Positive        Positive        Negative        ✗         \n",
      "This movie was terrible                                      Negative        Positive        Negative        ✗         \n",
      "This is not bad                                              Positive        Positive        Negative        ✗         \n",
      "Good movie but bad acting                                    Positive        Positive        Negative        ✗         \n",
      "Despite the poor beginning, the ending was great             Positive        Positive        Negative        ✗         \n",
      "The plot was intricate and the characters were well develope... Positive        Positive        Negative        ✗         \n",
      "A masterpiece of modern cinema with stunning visuals         Positive        Positive        Negative        ✗         \n",
      "The director failed to engage the audience                   Negative        Positive        Negative        ✗         \n",
      "Not the best film I've seen, but still enjoyable             Positive        Positive        Negative        ✗         \n",
      "I wouldn't recommend this to anyone                          Negative        Positive        Positive        ✓         \n",
      "It wasn't as bad as the critics suggested                    Positive        Positive        Negative        ✗         \n",
      "Absolutely brilliant performances by the entire cast         Positive        Positive        Negative        ✗         \n",
      "A complete waste of time and money                           Negative        Positive        Negative        ✗         \n",
      "The special effects couldn't save the weak storyline         Negative        Positive        Negative        ✗         \n",
      "Despite its flaws, the film manages to be entertaining       Positive        Positive        Negative        ✗         \n",
      "It's so bad it's actually good                               Positive        Positive        Negative        ✗         \n",
      "The film offers nothing new to the genre                     Negative        Positive        Negative        ✗         \n",
      "While not perfect, it exceeded my expectations               Positive        Positive        Negative        ✗         \n",
      "The soundtrack was the only redeeming quality                Negative        Positive        Negative        ✗         \n",
      "\n",
      "Fine-tuned model accuracy: 15/24 (62.5%)\n",
      "Base model accuracy: 7/24 (29.2%)\n",
      "Higher accuracy: Fine-tuned\n",
      "\n",
      "Agreement between models: 4/24 (16.7%)\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions between fine-tuned and base models\n",
    "test_examples = [\n",
    "    (\"Bad\", \"Negative\"),\n",
    "    (\"Good\", \"Positive\"),\n",
    "    (\"I hate this\", \"Negative\"),\n",
    "    (\"I love this\", \"Positive\"),\n",
    "    (\"This movie was OK\", \"Positive\"),\n",
    "    (\"This movie was fantastic\", \"Positive\"),\n",
    "    (\"This movie was terrible\", \"Negative\"),\n",
    "    (\"This is not bad\", \"Positive\"),\n",
    "    (\"Good movie but bad acting\", \"Positive\"),\n",
    "    (\"Despite the poor beginning, the ending was great\", \"Positive\"),\n",
    "    (\"The plot was intricate and the characters were well developed\", \"Positive\"),\n",
    "    (\"A masterpiece of modern cinema with stunning visuals\", \"Positive\"),    \n",
    "    (\"The director failed to engage the audience\", \"Negative\"),\n",
    "    (\"Not the best film I've seen, but still enjoyable\", \"Positive\"),\n",
    "    (\"I wouldn't recommend this to anyone\", \"Negative\"),\n",
    "    (\"It wasn't as bad as the critics suggested\", \"Positive\"),\n",
    "    (\"Absolutely brilliant performances by the entire cast\", \"Positive\"),\n",
    "    (\"A complete waste of time and money\", \"Negative\"),\n",
    "    (\"The special effects couldn't save the weak storyline\", \"Negative\"),\n",
    "    (\"Despite its flaws, the film manages to be entertaining\", \"Positive\"),\n",
    "    (\"It's so bad it's actually good\", \"Positive\"),\n",
    "    (\"The film offers nothing new to the genre\", \"Negative\"),\n",
    "    (\"While not perfect, it exceeded my expectations\", \"Positive\"),\n",
    "    (\"The soundtrack was the only redeeming quality\", \"Negative\")\n",
    "]\n",
    "\n",
    "print(\"\\nComparing predictions between fine-tuned and base models:\")\n",
    "print(f\"{'Text':<60} {'Ground Truth':<15} {'Fine-tuned':<15} {'Base model':<15} {'Match?':<10}\")\n",
    "print(\"-\" * 115)\n",
    "\n",
    "matches = 0\n",
    "fine_tuned_correct = 0\n",
    "base_correct = 0\n",
    "for example, ground_truth in test_examples:\n",
    "    fine_tuned_pred = predict_sentiment(example)\n",
    "    base_pred = predict_sentiment_base(example)\n",
    "    match = \"✓\" if fine_tuned_pred == base_pred else \"✗\"\n",
    "    if fine_tuned_pred == base_pred:\n",
    "        matches += 1\n",
    "    \n",
    "    # Count correct predictions (excluding Mixed/Neutral cases)\n",
    "    if ground_truth in [\"Positive\", \"Negative\"]:\n",
    "        if fine_tuned_pred == ground_truth:\n",
    "            fine_tuned_correct += 1\n",
    "        if base_pred == ground_truth:\n",
    "            base_correct += 1\n",
    "    \n",
    "    # Truncate long examples for display\n",
    "    display_text = example[:60] + \"...\" if len(example) > 60 else example\n",
    "    print(f\"{display_text:<60} {ground_truth:<15} {fine_tuned_pred:<15} {base_pred:<15} {match:<10}\")\n",
    "\n",
    "# Calculate metrics\n",
    "agreement_pct = (matches / len(test_examples)) * 100\n",
    "# Count binary sentiment examples (not Mixed or Neutral)\n",
    "binary_examples = sum(1 for _, label in test_examples if label in [\"Positive\", \"Negative\"])\n",
    "fine_tuned_acc = (fine_tuned_correct / binary_examples) * 100 if binary_examples > 0 else 0\n",
    "base_acc = (base_correct / binary_examples) * 100 if binary_examples > 0 else 0\n",
    "\n",
    "# Determine which model is better\n",
    "better_model = \"Fine-tuned\" if fine_tuned_acc > base_acc else \"Base\" if base_acc > fine_tuned_acc else \"Both equal\"\n",
    "\n",
    "print(f\"\\nFine-tuned model accuracy: {fine_tuned_correct}/{binary_examples} ({fine_tuned_acc:.1f}%)\")\n",
    "print(f\"Base model accuracy: {base_correct}/{binary_examples} ({base_acc:.1f}%)\")\n",
    "print(f\"Higher accuracy: {better_model}\")\n",
    "print(f\"\\nAgreement between models: {matches}/{len(test_examples)} ({agreement_pct:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs681-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
