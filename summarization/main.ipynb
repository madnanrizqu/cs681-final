{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"03230fb27ddf4eef957d52feadb0b89b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f4905b921c1491cadae2b9c12225039":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93682f2ea2624c20a290e049a0a9a7a6","IPY_MODEL_ee01a36eb33547ed93c4a0151fad34dc","IPY_MODEL_c80a3a213dcb472d97853828dc98b3ef"],"layout":"IPY_MODEL_81965205ebf64cd7aacdf3a76cecb6a0"}},"22c09ca25d2140f0a57f5794e365dd15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"333d95d0522943a1a626497184d1a347":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"645d7a349cbe4ca4859f4e014fa0e6a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81965205ebf64cd7aacdf3a76cecb6a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ebb544dcaf845b6ad10311f475a6792":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93682f2ea2624c20a290e049a0a9a7a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03230fb27ddf4eef957d52feadb0b89b","placeholder":"​","style":"IPY_MODEL_22c09ca25d2140f0a57f5794e365dd15","value":"Loading checkpoint shards: 100%"}},"ba3240daf67d45ca886b836c67a3fbf0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c80a3a213dcb472d97853828dc98b3ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba3240daf67d45ca886b836c67a3fbf0","placeholder":"​","style":"IPY_MODEL_645d7a349cbe4ca4859f4e014fa0e6a9","value":" 2/2 [00:20&lt;00:00,  9.04s/it]"}},"ee01a36eb33547ed93c4a0151fad34dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ebb544dcaf845b6ad10311f475a6792","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_333d95d0522943a1a626497184d1a347","value":2}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:39.716339Z","iopub.execute_input":"2025-05-12T21:11:39.716530Z","iopub.status.idle":"2025-05-12T21:11:39.720276Z","shell.execute_reply.started":"2025-05-12T21:11:39.716514Z","shell.execute_reply":"2025-05-12T21:11:39.719462Z"},"id":"jh1QCWE5OFu9","trusted":true},"outputs":[],"execution_count":95},{"cell_type":"code","source":"!pip install datasets trl bitsandbytes accelerate rouge_score evaluate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:39.721106Z","iopub.execute_input":"2025-05-12T21:11:39.721901Z","iopub.status.idle":"2025-05-12T21:11:43.218882Z","shell.execute_reply.started":"2025-05-12T21:11:39.721878Z","shell.execute_reply":"2025-05-12T21:11:43.217918Z"},"id":"MVd13x8IPm8B","outputId":"9d4bf7f8-fe3c-47cd-e68b-4b22ea1c63c2","trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.17.0)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.51.1)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:43.220143Z","iopub.execute_input":"2025-05-12T21:11:43.220413Z","iopub.status.idle":"2025-05-12T21:11:43.225526Z","shell.execute_reply.started":"2025-05-12T21:11:43.220391Z","shell.execute_reply":"2025-05-12T21:11:43.224748Z"},"id":"DmFrYP6LPkEv","trusted":true},"outputs":[],"execution_count":97},{"cell_type":"code","source":"# Global seed definition for reproducibility\nGLOBAL_SEED = 42\n\n# Set seeds for all relevant libraries\nimport numpy as np\nimport torch\nimport random\nfrom transformers import set_seed\n\n# Set seeds for Python's random module\nrandom.seed(GLOBAL_SEED)\n\n# Set seeds for NumPy\nnp.random.seed(GLOBAL_SEED)\n\n# Set seeds for PyTorch\ntorch.manual_seed(GLOBAL_SEED)\ntorch.cuda.manual_seed_all(GLOBAL_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Set seeds for Transformers\nset_seed(GLOBAL_SEED)\n\nprint(f\"All random seeds have been set to {GLOBAL_SEED} for reproducibility.\")","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:43.226386Z","iopub.execute_input":"2025-05-12T21:11:43.226643Z","iopub.status.idle":"2025-05-12T21:11:43.245102Z","shell.execute_reply.started":"2025-05-12T21:11:43.226622Z","shell.execute_reply":"2025-05-12T21:11:43.244344Z"},"trusted":true},"outputs":[{"name":"stdout","text":"All random seeds have been set to 42 for reproducibility.\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:43.245925Z","iopub.execute_input":"2025-05-12T21:11:43.246423Z","iopub.status.idle":"2025-05-12T21:11:43.258329Z","shell.execute_reply.started":"2025-05-12T21:11:43.246399Z","shell.execute_reply":"2025-05-12T21:11:43.257666Z"},"id":"M8N1oPk5SYI2","trusted":true},"outputs":[],"execution_count":99},{"cell_type":"code","source":"# https://huggingface.co/datasets/abisee/cnn_dailymail\nhuggingface_dataset_name = \"abisee/cnn_dailymail\"\ndataset = load_dataset(huggingface_dataset_name, \"3.0.0\")  # Using version 3.0.0 (non-anonymized)\ndataset","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:43.259144Z","iopub.execute_input":"2025-05-12T21:11:43.259899Z","iopub.status.idle":"2025-05-12T21:11:47.192468Z","shell.execute_reply.started":"2025-05-12T21:11:43.259876Z","shell.execute_reply":"2025-05-12T21:11:47.191758Z"},"id":"ldFhRMQcQQuH","outputId":"62bc3383-a368-4506-e1d1-d7d86ba4bf9f","trusted":true},"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"},"metadata":{}}],"execution_count":100},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:47.193501Z","iopub.execute_input":"2025-05-12T21:11:47.193821Z","iopub.status.idle":"2025-05-12T21:11:47.199475Z","shell.execute_reply.started":"2025-05-12T21:11:47.193796Z","shell.execute_reply":"2025-05-12T21:11:47.198740Z"},"id":"c78xfmCNQYey","outputId":"46982e88-71d6-46e9-8712-f501dbd9c776","trusted":true},"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"{'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}"},"metadata":{}}],"execution_count":101},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:47.200313Z","iopub.execute_input":"2025-05-12T21:11:47.200972Z","iopub.status.idle":"2025-05-12T21:11:47.223641Z","shell.execute_reply.started":"2025-05-12T21:11:47.200945Z","shell.execute_reply":"2025-05-12T21:11:47.222947Z"},"id":"OExNdyBfQbuR","trusted":true},"outputs":[],"execution_count":102},{"cell_type":"code","source":"model_name='microsoft/phi-2'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name,\n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["0f4905b921c1491cadae2b9c12225039","93682f2ea2624c20a290e049a0a9a7a6","ee01a36eb33547ed93c4a0151fad34dc","c80a3a213dcb472d97853828dc98b3ef","81965205ebf64cd7aacdf3a76cecb6a0","03230fb27ddf4eef957d52feadb0b89b","22c09ca25d2140f0a57f5794e365dd15","8ebb544dcaf845b6ad10311f475a6792","333d95d0522943a1a626497184d1a347","ba3240daf67d45ca886b836c67a3fbf0","645d7a349cbe4ca4859f4e014fa0e6a9"]},"execution":{"iopub.status.busy":"2025-05-12T21:11:47.224420Z","iopub.execute_input":"2025-05-12T21:11:47.224714Z","iopub.status.idle":"2025-05-12T21:11:54.031162Z","shell.execute_reply.started":"2025-05-12T21:11:47.224685Z","shell.execute_reply":"2025-05-12T21:11:54.030342Z"},"id":"rNi14jQjQ4b5","outputId":"2c32021b-d4a1-4324-8a4b-185151366b76","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35831a1ad02e4c4eb4f32dab0a5f5ca8"}},"metadata":{}}],"execution_count":103},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:54.032028Z","iopub.execute_input":"2025-05-12T21:11:54.032313Z","iopub.status.idle":"2025-05-12T21:11:54.323429Z","shell.execute_reply.started":"2025-05-12T21:11:54.032265Z","shell.execute_reply":"2025-05-12T21:11:54.322736Z"},"id":"6_IZxFLUSCUg","trusted":true},"outputs":[],"execution_count":104},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:54.324212Z","iopub.execute_input":"2025-05-12T21:11:54.324428Z","iopub.status.idle":"2025-05-12T21:11:54.328757Z","shell.execute_reply.started":"2025-05-12T21:11:54.324411Z","shell.execute_reply":"2025-05-12T21:11:54.327960Z"},"id":"JHgyu7ZASRwB","outputId":"089cccba-e869-4081-e84e-a685dc7d7150","trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 9196 MB.\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model, p, maxlen=100, sample=True, seed=GLOBAL_SEED):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(\n        **toks.to(\"cuda\"), \n        max_new_tokens=maxlen, \n        do_sample=sample,\n        num_return_sequences=1,\n        temperature=0.1,\n        num_beams=1,\n        top_p=0.95,\n    ).to('cpu')\n    return eval_tokenizer.batch_decode(res, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:54.329766Z","iopub.execute_input":"2025-05-12T21:11:54.330214Z","iopub.status.idle":"2025-05-12T21:11:54.635476Z","shell.execute_reply.started":"2025-05-12T21:11:54.330192Z","shell.execute_reply":"2025-05-12T21:11:54.634922Z"},"id":"bd4liREwSkTw","trusted":true},"outputs":[],"execution_count":106},{"cell_type":"code","source":"%%time\nindex = 10\n\narticle = dataset['test'][index]['article']\nsummary = dataset['test'][index]['highlights']\n\nformatted_prompt = f\"Instruct: Summarize the following article.\\n{article}\\nOutput:\\n\"\nres = gen(original_model, formatted_prompt, 100,)\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:54.636197Z","iopub.execute_input":"2025-05-12T21:11:54.636462Z","iopub.status.idle":"2025-05-12T21:11:57.765232Z","shell.execute_reply.started":"2025-05-12T21:11:54.636439Z","shell.execute_reply":"2025-05-12T21:11:57.764426Z"},"id":"A_gmNFEVSzKB","outputId":"91503dcd-b140-408d-a8e2-ad8f2badf554","trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following article.\nLondon (CNN)A 19-year-old man was charged Wednesday with terror offenses after he was arrested as he returned to Britain from Turkey, London's Metropolitan Police said. Yahya Rashid, a UK national from northwest London, was detained at Luton airport on Tuesday after he arrived on a flight from Istanbul, police said. He's been charged with engaging in conduct in preparation of acts of terrorism, and with engaging in conduct with the intention of assisting others to commit acts of terrorism. Both charges relate to the period between November 1 and March 31. Rashid is due to appear in Westminster Magistrates' Court on Wednesday, police said. CNN's Lindsay Isaac contributed to this report.\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nLondon's Metropolitan Police say the man was arrested at Luton airport after landing on a flight from Istanbul .\nHe's been charged with terror offenses allegedly committed since the start of November .\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nA 19-year-old man from northwest London was arrested at Luton airport on Tuesday after returning to Britain from Turkey. He was charged with engaging in conduct in preparation of acts of terrorism and engaging in conduct with the intention of assisting others to commit acts of terrorism. The charges relate to the period between November 1 and March 31. He is due to appear in Westminster Magistrates' Court on Wednesday.\n\nCPU times: user 3.12 s, sys: 2.76 ms, total: 3.13 s\nWall time: 3.12 s\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"# filepath: /Users/madnanrizqu/Code/KAU/cs681-final/summarization/main.ipynb\ndef create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters\n    :param sample: Sample dictionary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below article.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n\n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['article']}\" if sample[\"article\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['highlights']}\"\n    end = f\"{END_KEY}\"\n\n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:57.766137Z","iopub.execute_input":"2025-05-12T21:11:57.766398Z","iopub.status.idle":"2025-05-12T21:11:57.771318Z","shell.execute_reply.started":"2025-05-12T21:11:57.766380Z","shell.execute_reply":"2025-05-12T21:11:57.770646Z"},"id":"cr-2z2t_TACP","trusted":true},"outputs":[],"execution_count":108},{"cell_type":"code","source":"def get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:57.772052Z","iopub.execute_input":"2025-05-12T21:11:57.772252Z","iopub.status.idle":"2025-05-12T21:11:57.786588Z","shell.execute_reply.started":"2025-05-12T21:11:57.772238Z","shell.execute_reply":"2025-05-12T21:11:57.785964Z"},"id":"EqV7-1W-TEy5","trusted":true},"outputs":[],"execution_count":109},{"cell_type":"code","source":"# Combine all splits into one dataset\ncombined_dataset = {\n    'train': dataset['train'],\n    'validation': dataset['validation'],\n    'test': dataset['test']\n}\n\n# Get the total size\ntrain_size = len(combined_dataset['train'])\nvalidation_size = len(combined_dataset['validation'])\ntest_size = len(combined_dataset['test'])\ntotal_size = train_size + validation_size + test_size\n\nprint(f\"Total dataset size: {total_size}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:57.792700Z","iopub.execute_input":"2025-05-12T21:11:57.792922Z","iopub.status.idle":"2025-05-12T21:11:57.806349Z","shell.execute_reply.started":"2025-05-12T21:11:57.792894Z","shell.execute_reply":"2025-05-12T21:11:57.805741Z"},"id":"X3AIrn45_k0M","outputId":"5929e9b2-576c-43f6-a5c6-7b3fff345cdf","trusted":true},"outputs":[{"name":"stdout","text":"Total dataset size: 311971\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"# Calculate new split sizes\nnew_train_size = int(0.70 * total_size)\nnew_val_size = int(0.15 * total_size)\nnew_dev_size = int(0.10 * total_size)\nnew_test_size = int(0.05 * total_size)\n\nprint(f\"New train size: {new_train_size}\")\nprint(f\"New validation size: {new_val_size}\")\nprint(f\"New dev size: {new_dev_size}\")\nprint(f\"New test size: {new_test_size}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:57.807034Z","iopub.execute_input":"2025-05-12T21:11:57.807206Z","iopub.status.idle":"2025-05-12T21:11:57.822305Z","shell.execute_reply.started":"2025-05-12T21:11:57.807193Z","shell.execute_reply":"2025-05-12T21:11:57.821518Z"},"id":"h0x4tWjm_k0M","outputId":"ac4c90d4-a8a9-44a9-ea35-a2662b333b91","trusted":true},"outputs":[{"name":"stdout","text":"New train size: 218379\nNew validation size: 46795\nNew dev size: 31197\nNew test size: 15598\n","output_type":"stream"}],"execution_count":111},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\n# Concatenate all splits into a single dataset\nall_data = concatenate_datasets([\n    dataset['train'],\n    dataset['validation'],\n    dataset['test']\n])\n\n# Ensure deterministic shuffling using GLOBAL_SEED\nshuffled_indices = np.random.permutation(len(all_data))\nall_data = all_data.select(shuffled_indices)\n\n# Create new splits\nnew_train_size = int(0.70 * len(all_data))\nnew_val_size = int(0.15 * len(all_data))\nnew_dev_size = int(0.10 * len(all_data))\nnew_test_size = int(0.05 * len(all_data))\n\n# Create new splits\nnew_train_dataset = all_data.select(range(0, new_train_size))\nnew_val_dataset = all_data.select(range(new_train_size, new_train_size + new_val_size))\nnew_dev_dataset = all_data.select(range(new_train_size + new_val_size, new_train_size + new_val_size + new_dev_size))\nnew_test_dataset = all_data.select(range(new_train_size + new_val_size + new_dev_size, new_train_size + new_val_size + new_dev_size + new_test_size))\n\nprint(f\"New train dataset size: {len(new_train_dataset)}\")\nprint(f\"New validation dataset size: {len(new_val_dataset)}\")\nprint(f\"New dev dataset size: {len(new_dev_dataset)}\")\nprint(f\"New test dataset size: {len(new_test_dataset)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:57.823146Z","iopub.execute_input":"2025-05-12T21:11:57.823403Z","iopub.status.idle":"2025-05-12T21:11:57.926440Z","shell.execute_reply.started":"2025-05-12T21:11:57.823381Z","shell.execute_reply":"2025-05-12T21:11:57.925754Z"},"id":"u_UzPloJ_k0M","outputId":"ed5f646e-5fff-4b75-edeb-add60963d428","trusted":true},"outputs":[{"name":"stdout","text":"New train dataset size: 218379\nNew validation dataset size: 46795\nNew dev dataset size: 31197\nNew test dataset size: 15598\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"from functools import partial\n\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n\n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)\n\n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'article', 'highlights'],  # Updated column names\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:11:57.927150Z","iopub.execute_input":"2025-05-12T21:11:57.927329Z","iopub.status.idle":"2025-05-12T21:11:57.932341Z","shell.execute_reply.started":"2025-05-12T21:11:57.927315Z","shell.execute_reply":"2025-05-12T21:11:57.931556Z"},"id":"khyBNnMOTNYY","trusted":true},"outputs":[],"execution_count":113},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:57.933131Z","iopub.execute_input":"2025-05-12T21:11:57.933454Z","iopub.status.idle":"2025-05-12T21:11:57.952203Z","shell.execute_reply.started":"2025-05-12T21:11:57.933430Z","shell.execute_reply":"2025-05-12T21:11:57.951487Z"},"id":"VypP9B9MTT-Q","outputId":"05379bf9-3959-4d2b-e63b-397b83d8e189","trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 9266 MB.\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"# Preprocess all splits\nmax_length = get_max_length(original_model)\nprint(f\"Using max length: {max_length}\")\n\n# Process each split using the global seed\ntrain_dataset = preprocess_dataset(tokenizer, max_length, GLOBAL_SEED, new_train_dataset.select(range(210)))\neval_dataset = preprocess_dataset(tokenizer, max_length, GLOBAL_SEED, new_val_dataset.select(range(45)))\ndev_dataset = preprocess_dataset(tokenizer, max_length, GLOBAL_SEED, new_dev_dataset.select(range(30)))\ntest_dataset = preprocess_dataset(tokenizer, max_length, GLOBAL_SEED, new_test_dataset.select(range(15)))\n\n# Print dataset shapes\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(f\"Dev: {dev_dataset.shape}\")\nprint(f\"Test: {test_dataset.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:11:57.952883Z","iopub.execute_input":"2025-05-12T21:11:57.953144Z","iopub.status.idle":"2025-05-12T21:12:01.080651Z","shell.execute_reply.started":"2025-05-12T21:11:57.953122Z","shell.execute_reply":"2025-05-12T21:12:01.079938Z"},"id":"Fn5T8UCPTXqd","outputId":"b36b18d5-ea3d-434e-cba4-b1eeeec02762","trusted":true},"outputs":[{"name":"stdout","text":"Found max lenth: 2048\nUsing max length: 2048\nPreprocessing dataset...\nPreprocessing dataset...\nPreprocessing dataset...\nPreprocessing dataset...\nShapes of the datasets:\nTraining: (203, 3)\nValidation: (44, 3)\nDev: (28, 3)\nTest: (15, 3)\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:12:01.081480Z","iopub.execute_input":"2025-05-12T21:12:01.082149Z","iopub.status.idle":"2025-05-12T21:12:01.086331Z","shell.execute_reply.started":"2025-05-12T21:12:01.082129Z","shell.execute_reply":"2025-05-12T21:12:01.085487Z"},"id":"G9rpKh-YTwSO","outputId":"6cdfcbed-56ff-4ef7-c26b-8bdabb2a4465","trusted":true},"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (203, 3)\nValidation: (44, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 203\n})\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:12:01.087153Z","iopub.execute_input":"2025-05-12T21:12:01.087465Z","iopub.status.idle":"2025-05-12T21:12:01.101800Z","shell.execute_reply.started":"2025-05-12T21:12:01.087447Z","shell.execute_reply":"2025-05-12T21:12:01.101077Z"},"id":"yuoK0dCkT0az","outputId":"0a7e60de-ce3c-45e2-8bca-10afe480df43","trusted":true},"outputs":[{"name":"stdout","text":"trainable model parameters: 262364160\nall model parameters: 1521392640\npercentage of trainable model parameters: 17.24%\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"print(original_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:12:01.102628Z","iopub.execute_input":"2025-05-12T21:12:01.103106Z","iopub.status.idle":"2025-05-12T21:12:01.123440Z","shell.execute_reply.started":"2025-05-12T21:12:01.103086Z","shell.execute_reply":"2025-05-12T21:12:01.122680Z"},"id":"IOm68qZZT4q8","outputId":"ebc0fc75-f765-4729-e10e-7874d10da5e0","trusted":true},"outputs":[{"name":"stdout","text":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (rotary_emb): PhiRotaryEmbedding()\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:12:01.124324Z","iopub.execute_input":"2025-05-12T21:12:01.124650Z","iopub.status.idle":"2025-05-12T21:12:01.481967Z","shell.execute_reply.started":"2025-05-12T21:12:01.124633Z","shell.execute_reply":"2025-05-12T21:12:01.481208Z"},"id":"djYOCCntT7rK","trusted":true},"outputs":[],"execution_count":119},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:12:01.483013Z","iopub.execute_input":"2025-05-12T21:12:01.483768Z","iopub.status.idle":"2025-05-12T21:12:01.490520Z","shell.execute_reply.started":"2025-05-12T21:12:01.483745Z","shell.execute_reply":"2025-05-12T21:12:01.489775Z"},"id":"x8gOrr9IUG8X","outputId":"07b801c3-1ba4-4e24-976b-e569196866b6","trusted":true},"outputs":[{"name":"stdout","text":"trainable model parameters: 20971520\nall model parameters: 1542364160\npercentage of trainable model parameters: 1.36%\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"print(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:12:01.491240Z","iopub.execute_input":"2025-05-12T21:12:01.491427Z","iopub.status.idle":"2025-05-12T21:12:01.512012Z","shell.execute_reply.started":"2025-05-12T21:12:01.491412Z","shell.execute_reply":"2025-05-12T21:12:01.511349Z"},"id":"i27yzRadUKUN","outputId":"6a2c6de8-b212-4795-bf38-18b233844c2c","trusted":true},"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PhiForCausalLM(\n      (model): PhiModel(\n        (embed_tokens): Embedding(51200, 2560)\n        (layers): ModuleList(\n          (0-31): 32 x PhiDecoderLayer(\n            (self_attn): PhiAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (dense): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): PhiMLP(\n              (activation_fn): NewGELUActivation()\n              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n            )\n            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (rotary_emb): PhiRotaryEmbedding()\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=10,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=10,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=10,\n    num_train_epochs=1,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n    seed=GLOBAL_SEED,  # Add global seed\n    data_seed=GLOBAL_SEED  # Add data seed for data loaders\n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:12:01.512768Z","iopub.execute_input":"2025-05-12T21:12:01.513030Z","iopub.status.idle":"2025-05-12T21:12:01.563443Z","shell.execute_reply.started":"2025-05-12T21:12:01.513007Z","shell.execute_reply":"2025-05-12T21:12:01.562854Z"},"id":"ijGOIAJAUQjK","outputId":"b361a160-805e-4d39-9469-95f1835e824f","trusted":true},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"peft_training_args.device","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T21:12:01.564162Z","iopub.execute_input":"2025-05-12T21:12:01.564653Z","iopub.status.idle":"2025-05-12T21:12:01.569067Z","shell.execute_reply.started":"2025-05-12T21:12:01.564628Z","shell.execute_reply":"2025-05-12T21:12:01.568338Z"},"id":"koz0-4PHUZzX","outputId":"8439479e-1eed-4b12-86b0-c539e6cc6fef","trusted":true},"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":123},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2025-05-12T21:12:01.570063Z","iopub.execute_input":"2025-05-12T21:12:01.570337Z","iopub.status.idle":"2025-05-12T21:15:36.784373Z","shell.execute_reply.started":"2025-05-12T21:12:01.570314Z","shell.execute_reply":"2025-05-12T21:15:36.783542Z"},"id":"FFcxA1B-UfJ1","outputId":"61ea5329-96a8-47f7-d188-f68db5d163fb","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 03:09, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.437600</td>\n      <td>2.414258</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=2.4376129150390624, metrics={'train_runtime': 214.3319, 'train_samples_per_second': 0.187, 'train_steps_per_second': 0.047, 'total_flos': 667241004165120.0, 'train_loss': 2.4376129150390624, 'epoch': 0.19704433497536947})"},"metadata":{}}],"execution_count":124},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:15:36.785307Z","iopub.execute_input":"2025-05-12T21:15:36.785609Z","iopub.status.idle":"2025-05-12T21:15:36.791745Z","shell.execute_reply.started":"2025-05-12T21:15:36.785583Z","shell.execute_reply":"2025-05-12T21:15:36.790917Z"},"id":"QajO3VMeXWAV","trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 14952 MB.\n","output_type":"stream"}],"execution_count":125},{"cell_type":"code","source":"# Step 8: Evaluate on Dev Split\nprint(\"Evaluating on Dev split...\")\ndev_results = peft_trainer.evaluate(dev_dataset)\nprint(f\"Dev set evaluation results:\")\nprint(dev_results)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T21:15:36.792645Z","iopub.execute_input":"2025-05-12T21:15:36.792902Z","iopub.status.idle":"2025-05-12T21:16:20.730245Z","shell.execute_reply.started":"2025-05-12T21:15:36.792881Z","shell.execute_reply":"2025-05-12T21:16:20.729277Z"},"id":"oWuUsgZY_k0O","trusted":true},"outputs":[{"name":"stdout","text":"Evaluating on Dev split...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:59]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Dev set evaluation results:\n{'eval_loss': 2.339982748031616, 'eval_runtime': 43.8914, 'eval_samples_per_second': 0.638, 'eval_steps_per_second': 0.091, 'epoch': 0.19704433497536947}\n","output_type":"stream"}],"execution_count":126},{"cell_type":"code","source":"# Step 9: Final Evaluation on Test Split (only run this once training is complete)\nprint(\"Performing final evaluation on Test split...\")\ntest_results = peft_trainer.evaluate(test_dataset)\nprint(f\"Final test set evaluation results:\")\nprint(test_results)","metadata":{"id":"ZgraAjRO_k0O","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:16:20.731222Z","iopub.execute_input":"2025-05-12T21:16:20.731592Z","iopub.status.idle":"2025-05-12T21:16:51.784400Z","shell.execute_reply.started":"2025-05-12T21:16:20.731561Z","shell.execute_reply":"2025-05-12T21:16:51.783670Z"}},"outputs":[{"name":"stdout","text":"Performing final evaluation on Test split...\nFinal test set evaluation results:\n{'eval_loss': 2.446943998336792, 'eval_runtime': 31.0249, 'eval_samples_per_second': 0.483, 'eval_steps_per_second': 0.064, 'epoch': 0.19704433497536947}\n","output_type":"stream"}],"execution_count":127},{"cell_type":"code","source":"# Free memory for merging weights\nif peft_trainer:\n  del peft_trainer\n\ntorch.cuda.empty_cache()","metadata":{"id":"34AMrQQ-ZXcl","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:16:51.785129Z","iopub.execute_input":"2025-05-12T21:16:51.785400Z","iopub.status.idle":"2025-05-12T21:16:51.887849Z","shell.execute_reply.started":"2025-05-12T21:16:51.785376Z","shell.execute_reply":"2025-05-12T21:16:51.887080Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"id":"WWvkFhcYZa4H","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:16:51.888816Z","iopub.execute_input":"2025-05-12T21:16:51.889103Z","iopub.status.idle":"2025-05-12T21:16:51.893680Z","shell.execute_reply.started":"2025-05-12T21:16:51.889081Z","shell.execute_reply":"2025-05-12T21:16:51.893022Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 5976 MB.\n","output_type":"stream"}],"execution_count":129},{"cell_type":"code","source":"import pandas as pd\nimport evaluate\nimport torch\nimport numpy as np\n\narticles = new_test_dataset[15:25]['article']\nhuman_baseline_summaries = new_test_dataset[15:25]['highlights']\n\noriginal_model_summaries = []\npeft_model_summaries = []\n\nfor idx, article in enumerate(articles):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    # Use prompt format consistent with training\n    prompt = f\"\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruct: Summarize the below article.\\n\\n{article}\\n\\n### Output:\\n\"\n\n    # Increase token limit\n    original_model_res = gen(original_model, prompt, 150)\n    \n    # More robust parsing with error handling\n    try:\n        if '### Output:' in original_model_res[0]:\n            original_model_text_output = original_model_res[0].split('### Output:')[1].strip()\n        elif 'Output:' in original_model_res[0]:\n            original_model_text_output = original_model_res[0].split('Output:')[1].strip()\n        else:\n            original_model_text_output = original_model_res[0]\n    except Exception as e:\n        print(f\"Error parsing original model output for article {idx}: {e}\")\n        original_model_text_output = \"[Parsing error]\"\n\n    # Same for PEFT model\n    peft_model_res = gen(peft_model, prompt, 150)\n    \n    try:\n        if '### Output:' in peft_model_res[0]:\n            peft_model_output = peft_model_res[0].split('### Output:')[1].strip()\n        elif 'Output:' in peft_model_res[0]:\n            peft_model_output = peft_model_res[0].split('Output:')[1].strip()\n        else:\n            peft_model_output = peft_model_res[0]\n            \n        # Handle various endings\n        for marker in ['### End', '#End', '##OUTPUT', '##End']:\n            if marker in peft_model_output:\n                peft_model_text_output = peft_model_output.split(marker)[0].strip()\n                break\n        else:  # No marker found\n            peft_model_text_output = peft_model_output.strip()\n            \n    except Exception as e:\n        print(f\"Error parsing PEFT model output for article {idx}: {e}\")\n        peft_model_text_output = \"[Parsing error]\"\n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n\ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"id":"EIPSiTRWbJs3","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:16:51.894556Z","iopub.execute_input":"2025-05-12T21:16:51.894806Z","iopub.status.idle":"2025-05-12T21:20:33.953280Z","shell.execute_reply.started":"2025-05-12T21:16:51.894785Z","shell.execute_reply":"2025-05-12T21:20:33.952569Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  France announces new ministers .\\nThe governme...   \n1  Miss California USA Carrie Prejean says she po...   \n2  Jay-Z reported to be 'deeply disappointed' by ...   \n3  Carbisdale Castle near Culrain is nestled on p...   \n4  Police collated the four-page dossier followin...   \n5  She still owns £3.9 million stately home in Ea...   \n6  Mitt Romney is trying to outdo President Obama...   \n7  Archaeologists believe site was centre of a ne...   \n8  Edward Snowden claimed that a damaging culture...   \n9  Enterovirus D68 is sending children with breat...   \n\n                            original_model_summaries  \\\n0  A new French government was announced Tuesday,...   \n1  A second lingerie-modeling photo of Miss Calif...   \n2  Rihanna was pictured out in London today looki...   \n3  Carbisdale Castle, near the village of Culrain...   \n4  An unnamed hit band from the 1960s attended th...   \n5  Heather Mills has sold her British beachfront ...   \n6  The article is about how Mitt Romney is trying...   \n7  The article describes how archaeologists have ...   \n8  Edward Snowden, who worked as a contract emplo...   \n9  Enterovirus D68 has swept through 30 states si...   \n\n                                peft_model_summaries  \n0  A new French government was announced on Tuesd...  \n1  A second lingerie-modeling photo of Miss Calif...  \n2  Rihanna was pictured out in London today looki...  \n3  Carbisdale Castle, near the Highland village o...  \n4  An unnamed hit band from the 1960s attended th...  \n5  Heather Mills has sold her British beachfront ...  \n6  The article is about how Mitt Romney is trying...  \n7  The article discusses the latest discovery of ...  \n8  Edward Snowden, who worked as a contract emplo...  \n9  Enterovirus D68 has swept through 30 states si...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>France announces new ministers .\\nThe governme...</td>\n      <td>A new French government was announced Tuesday,...</td>\n      <td>A new French government was announced on Tuesd...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Miss California USA Carrie Prejean says she po...</td>\n      <td>A second lingerie-modeling photo of Miss Calif...</td>\n      <td>A second lingerie-modeling photo of Miss Calif...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jay-Z reported to be 'deeply disappointed' by ...</td>\n      <td>Rihanna was pictured out in London today looki...</td>\n      <td>Rihanna was pictured out in London today looki...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Carbisdale Castle near Culrain is nestled on p...</td>\n      <td>Carbisdale Castle, near the village of Culrain...</td>\n      <td>Carbisdale Castle, near the Highland village o...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Police collated the four-page dossier followin...</td>\n      <td>An unnamed hit band from the 1960s attended th...</td>\n      <td>An unnamed hit band from the 1960s attended th...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>She still owns £3.9 million stately home in Ea...</td>\n      <td>Heather Mills has sold her British beachfront ...</td>\n      <td>Heather Mills has sold her British beachfront ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Mitt Romney is trying to outdo President Obama...</td>\n      <td>The article is about how Mitt Romney is trying...</td>\n      <td>The article is about how Mitt Romney is trying...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Archaeologists believe site was centre of a ne...</td>\n      <td>The article describes how archaeologists have ...</td>\n      <td>The article discusses the latest discovery of ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Edward Snowden claimed that a damaging culture...</td>\n      <td>Edward Snowden, who worked as a contract emplo...</td>\n      <td>Edward Snowden, who worked as a contract emplo...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Enterovirus D68 is sending children with breat...</td>\n      <td>Enterovirus D68 has swept through 30 states si...</td>\n      <td>Enterovirus D68 has swept through 30 states si...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":130},{"cell_type":"code","source":"import evaluate\n# ROUGE evaluation\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\n# BLEU evaluation\nbleu = evaluate.load('bleu')\n\noriginal_model_bleu = bleu.compute(\n    predictions=original_model_summaries,\n    references=[[ref] for ref in human_baseline_summaries]\n)\n\npeft_model_bleu = bleu.compute(\n    predictions=peft_model_summaries, \n    references=[[ref] for ref in human_baseline_summaries]\n)\n\n# Fixed perplexity calculation function\ndef calculate_perplexity_alt(model, tokenizer, texts, max_loss=20.0):\n    perplexities = []\n    \n    with torch.no_grad():\n        for text in texts:\n            try:\n                # Tokenize without setting labels\n                inputs = tokenizer(text, return_tensors=\"pt\")\n                # Move to device\n                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n                \n                # Forward pass to get logits\n                outputs = model(**inputs)\n                logits = outputs.logits\n                \n                # Calculate loss manually\n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = inputs['input_ids'][..., 1:].contiguous()\n                \n                loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')\n                loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), \n                              shift_labels.reshape(-1))\n                \n                # Clip loss to prevent overflow\n                loss = torch.clamp(loss, 0, max_loss)\n                \n                # Calculate perplexity\n                perplexity = torch.exp(loss)\n                \n                if not torch.isnan(perplexity) and not torch.isinf(perplexity):\n                    perplexities.append(perplexity.item())\n            except Exception as e:\n                print(f\"Error processing text: {e}\")\n                continue\n    \n    if not perplexities:\n        return float('nan')\n    \n    return sum(perplexities) / len(perplexities)\n\n# Try the alternative calculation\noriginal_perplexity = calculate_perplexity_alt(original_model, eval_tokenizer, human_baseline_summaries)\npeft_perplexity = calculate_perplexity_alt(peft_model, eval_tokenizer, peft_model_summaries)\n\n# Helper function for formatting outputs\ndef format_metrics(rouge_results, bleu_results, perplexity):\n    print(\"=== ROUGE Scores ===\")\n    for metric, value in rouge_results.items():\n        print(f\"  {metric}: {value*100:.2f}%\")\n    \n    print(\"\\n=== BLEU Score ===\")\n    print(f\"  BLEU: {bleu_results['bleu']*100:.2f}%\")\n    print(\"  Precision by n-gram:\")\n    for n, precision in enumerate(bleu_results['precisions'], 1):\n        print(f\"    {n}-gram: {precision*100:.2f}%\")\n    \n    print(f\"\\n=== Perplexity ===\")\n    print(f\"  {perplexity:.4f} (lower is better)\")\n\n# Print results with better formatting\nprint(\"\\n\" + \"=\"*50)\nprint(\"ORIGINAL MODEL EVALUATION\")\nprint(\"=\"*50)\nformat_metrics(original_model_results, original_model_bleu, original_perplexity)\n\nprint(\"\\n\\n\" + \"=\"*50)\nprint(\"PEFT MODEL EVALUATION\")\nprint(\"=\"*50)\nformat_metrics(peft_model_results, peft_model_bleu, peft_perplexity)\n\n# Calculate and display improvements\nprint(\"\\n\\n\" + \"=\"*50)\nprint(\"IMPROVEMENT SUMMARY\")\nprint(\"=\"*50)\n\n# ROUGE improvement\nprint(\"=== ROUGE Improvement ===\")\nfor metric in original_model_results.keys():\n    improvement = (peft_model_results[metric] - original_model_results[metric]) * 100\n    print(f\"  {metric}: {improvement:.2f}% absolute improvement\")\n\n# BLEU improvement\nbleu_improvement = (peft_model_bleu['bleu'] - original_model_bleu['bleu']) * 100\nprint(\"\\n=== BLEU Improvement ===\")\nprint(f\"  {bleu_improvement:.2f}% absolute improvement\")\n\n# Perplexity improvement (lower is better)\nperplexity_improvement = original_perplexity - peft_perplexity\nperplexity_improvement_percent = (perplexity_improvement / original_perplexity) * 100\nprint(\"\\n=== Perplexity Improvement ===\")\nprint(f\"  {perplexity_improvement_percent:.2f}% reduction\")","metadata":{"id":"4VLiakHMbu_U","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:20:33.954099Z","iopub.execute_input":"2025-05-12T21:20:33.954370Z","iopub.status.idle":"2025-05-12T21:20:42.336314Z","shell.execute_reply.started":"2025-05-12T21:20:33.954348Z","shell.execute_reply":"2025-05-12T21:20:42.335624Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nORIGINAL MODEL EVALUATION\n==================================================\n=== ROUGE Scores ===\n  rouge1: 31.80%\n  rouge2: 9.84%\n  rougeL: 19.20%\n  rougeLsum: 23.78%\n\n=== BLEU Score ===\n  BLEU: 4.67%\n  Precision by n-gram:\n    1-gram: 22.34%\n    2-gram: 6.16%\n    3-gram: 2.87%\n    4-gram: 1.20%\n\n=== Perplexity ===\n  44.5138 (lower is better)\n\n\n==================================================\nPEFT MODEL EVALUATION\n==================================================\n=== ROUGE Scores ===\n  rouge1: 31.32%\n  rouge2: 10.37%\n  rougeL: 19.25%\n  rougeLsum: 24.23%\n\n=== BLEU Score ===\n  BLEU: 4.68%\n  Precision by n-gram:\n    1-gram: 21.94%\n    2-gram: 6.24%\n    3-gram: 2.80%\n    4-gram: 1.25%\n\n=== Perplexity ===\n  11.9945 (lower is better)\n\n\n==================================================\nIMPROVEMENT SUMMARY\n==================================================\n=== ROUGE Improvement ===\n  rouge1: -0.48% absolute improvement\n  rouge2: 0.54% absolute improvement\n  rougeL: 0.05% absolute improvement\n  rougeLsum: 0.45% absolute improvement\n\n=== BLEU Improvement ===\n  0.01% absolute improvement\n\n=== Perplexity Improvement ===\n  73.05% reduction\n","output_type":"stream"}],"execution_count":131}]}