{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"03230fb27ddf4eef957d52feadb0b89b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f4905b921c1491cadae2b9c12225039":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93682f2ea2624c20a290e049a0a9a7a6","IPY_MODEL_ee01a36eb33547ed93c4a0151fad34dc","IPY_MODEL_c80a3a213dcb472d97853828dc98b3ef"],"layout":"IPY_MODEL_81965205ebf64cd7aacdf3a76cecb6a0"}},"22c09ca25d2140f0a57f5794e365dd15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"333d95d0522943a1a626497184d1a347":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"645d7a349cbe4ca4859f4e014fa0e6a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81965205ebf64cd7aacdf3a76cecb6a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ebb544dcaf845b6ad10311f475a6792":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93682f2ea2624c20a290e049a0a9a7a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03230fb27ddf4eef957d52feadb0b89b","placeholder":"​","style":"IPY_MODEL_22c09ca25d2140f0a57f5794e365dd15","value":"Loading checkpoint shards: 100%"}},"ba3240daf67d45ca886b836c67a3fbf0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c80a3a213dcb472d97853828dc98b3ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba3240daf67d45ca886b836c67a3fbf0","placeholder":"​","style":"IPY_MODEL_645d7a349cbe4ca4859f4e014fa0e6a9","value":" 2/2 [00:20&lt;00:00,  9.04s/it]"}},"ee01a36eb33547ed93c4a0151fad34dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ebb544dcaf845b6ad10311f475a6792","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_333d95d0522943a1a626497184d1a347","value":2}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:13:15.208089Z","iopub.execute_input":"2025-05-12T20:13:15.208345Z","iopub.status.idle":"2025-05-12T20:13:15.215770Z","shell.execute_reply.started":"2025-05-12T20:13:15.208325Z","shell.execute_reply":"2025-05-12T20:13:15.215050Z"},"id":"jh1QCWE5OFu9","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install datasets trl bitsandbytes accelerate rouge_score evaluate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:13:15.217158Z","iopub.execute_input":"2025-05-12T20:13:15.217794Z","iopub.status.idle":"2025-05-12T20:14:43.838766Z","shell.execute_reply.started":"2025-05-12T20:13:15.217776Z","shell.execute_reply":"2025-05-12T20:14:43.838084Z"},"id":"MVd13x8IPm8B","outputId":"9d4bf7f8-fe3c-47cd-e68b-4b22ea1c63c2","trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting trl\n  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.51.1)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading trl-0.17.0-py3-none-any.whl (348 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=754a78590be2d835aca79e4a01b316a37657e847a3001cd3bbc883d43b1ca4b8\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trl, rouge_score, evaluate, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 evaluate-0.4.3 fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge_score-0.1.2 trl-0.17.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:14:43.839815Z","iopub.execute_input":"2025-05-12T20:14:43.840117Z","iopub.status.idle":"2025-05-12T20:15:10.475773Z","shell.execute_reply.started":"2025-05-12T20:14:43.840095Z","shell.execute_reply":"2025-05-12T20:15:10.475182Z"},"id":"DmFrYP6LPkEv","trusted":true},"outputs":[{"name":"stderr","text":"2025-05-12 20:14:58.163752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747080898.346487      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747080898.399507      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Global seed definition for reproducibility\nGLOBAL_SEED = 42\n\n# Set seeds for all relevant libraries\nimport numpy as np\nimport torch\nimport random\nfrom transformers import set_seed\n\n# Set seeds for Python's random module\nrandom.seed(GLOBAL_SEED)\n\n# Set seeds for NumPy\nnp.random.seed(GLOBAL_SEED)\n\n# Set seeds for PyTorch\ntorch.manual_seed(GLOBAL_SEED)\ntorch.cuda.manual_seed_all(GLOBAL_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Set seeds for Transformers\nset_seed(GLOBAL_SEED)\n\nprint(f\"All random seeds have been set to {GLOBAL_SEED} for reproducibility.\")","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:15:10.477709Z","iopub.execute_input":"2025-05-12T20:15:10.478278Z","iopub.status.idle":"2025-05-12T20:15:10.490986Z","shell.execute_reply.started":"2025-05-12T20:15:10.478259Z","shell.execute_reply":"2025-05-12T20:15:10.490424Z"},"trusted":true},"outputs":[{"name":"stdout","text":"All random seeds have been set to 42 for reproducibility.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:15:10.491710Z","iopub.execute_input":"2025-05-12T20:15:10.492003Z","iopub.status.idle":"2025-05-12T20:15:10.508413Z","shell.execute_reply.started":"2025-05-12T20:15:10.491978Z","shell.execute_reply":"2025-05-12T20:15:10.507829Z"},"id":"M8N1oPk5SYI2","trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# https://huggingface.co/datasets/abisee/cnn_dailymail\nhuggingface_dataset_name = \"abisee/cnn_dailymail\"\ndataset = load_dataset(huggingface_dataset_name, \"3.0.0\")  # Using version 3.0.0 (non-anonymized)\ndataset","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:15:10.509275Z","iopub.execute_input":"2025-05-12T20:15:10.509559Z","iopub.status.idle":"2025-05-12T20:15:24.451275Z","shell.execute_reply.started":"2025-05-12T20:15:10.509533Z","shell.execute_reply":"2025-05-12T20:15:24.450693Z"},"id":"ldFhRMQcQQuH","outputId":"62bc3383-a368-4506-e1d1-d7d86ba4bf9f","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad4824b97f8841ed9d3d9702531363ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef2e849d36c84688802993ad1ab5f4db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2a3d459a7a437eba7827e64808fa43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63dc70bb359e438084eb27c681861887"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ed61bac6ba40a6b86fcd06257153ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf49feefbaa498aa7fe54570fcaab9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a20286be5640c6a76141e44a7032eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c774c0ba0b47b08a54aaa2c721b4c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b27f509d5334bb496a1951ee8eb386b"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:15:24.452068Z","iopub.execute_input":"2025-05-12T20:15:24.452591Z","iopub.status.idle":"2025-05-12T20:15:24.457639Z","shell.execute_reply.started":"2025-05-12T20:15:24.452562Z","shell.execute_reply":"2025-05-12T20:15:24.456892Z"},"id":"c78xfmCNQYey","outputId":"46982e88-71d6-46e9-8712-f501dbd9c776","trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:15:24.458441Z","iopub.execute_input":"2025-05-12T20:15:24.459237Z","iopub.status.idle":"2025-05-12T20:15:24.477434Z","shell.execute_reply.started":"2025-05-12T20:15:24.459194Z","shell.execute_reply":"2025-05-12T20:15:24.476862Z"},"id":"OExNdyBfQbuR","trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model_name='microsoft/phi-2'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name,\n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["0f4905b921c1491cadae2b9c12225039","93682f2ea2624c20a290e049a0a9a7a6","ee01a36eb33547ed93c4a0151fad34dc","c80a3a213dcb472d97853828dc98b3ef","81965205ebf64cd7aacdf3a76cecb6a0","03230fb27ddf4eef957d52feadb0b89b","22c09ca25d2140f0a57f5794e365dd15","8ebb544dcaf845b6ad10311f475a6792","333d95d0522943a1a626497184d1a347","ba3240daf67d45ca886b836c67a3fbf0","645d7a349cbe4ca4859f4e014fa0e6a9"]},"execution":{"iopub.status.busy":"2025-05-12T20:15:24.478087Z","iopub.execute_input":"2025-05-12T20:15:24.478282Z","iopub.status.idle":"2025-05-12T20:15:50.929875Z","shell.execute_reply.started":"2025-05-12T20:15:24.478266Z","shell.execute_reply":"2025-05-12T20:15:50.929327Z"},"id":"rNi14jQjQ4b5","outputId":"2c32021b-d4a1-4324-8a4b-185151366b76","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b192718095454820ab1c48f6adc84aa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90310cc94a334c87a7bf0e5bf4b2611d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a2dac3094da49a2ac1e7f3c1dd80542"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f10fb65607449389667416cbf4d6f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1564156c4cd94144af115f8cb45f26fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28720291d7eb4a67af62f890a702ecf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d0d46c30d14af1a479633d0c64c083"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:15:50.932437Z","iopub.execute_input":"2025-05-12T20:15:50.932649Z","iopub.status.idle":"2025-05-12T20:15:55.314841Z","shell.execute_reply.started":"2025-05-12T20:15:50.932632Z","shell.execute_reply":"2025-05-12T20:15:55.314269Z"},"id":"6_IZxFLUSCUg","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa96fcb747c848e18503a45bb528deac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c5994df58e14fe5a83cdc549b221bf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fade9adea0674744877b542863735053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c0192a0e0a40e4ac7991f45d8af0b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e89084aed3434781b4aa3333ee3c539b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71316d0d6d1c4f18a8555f9c4499ae5f"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:15:55.315786Z","iopub.execute_input":"2025-05-12T20:15:55.316024Z","iopub.status.idle":"2025-05-12T20:16:01.932539Z","shell.execute_reply.started":"2025-05-12T20:15:55.316006Z","shell.execute_reply":"2025-05-12T20:16:01.931639Z"},"id":"JHgyu7ZASRwB","outputId":"089cccba-e869-4081-e84e-a685dc7d7150","trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 3104 MB.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model, p, maxlen=100, sample=True, seed=GLOBAL_SEED):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(\n        **toks.to(\"cuda\"), \n        max_new_tokens=maxlen, \n        do_sample=sample,\n        num_return_sequences=1,\n        temperature=0.1,\n        num_beams=1,\n        top_p=0.95,\n    ).to('cpu')\n    return eval_tokenizer.batch_decode(res, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:16:01.933533Z","iopub.execute_input":"2025-05-12T20:16:01.933818Z","iopub.status.idle":"2025-05-12T20:16:02.228064Z","shell.execute_reply.started":"2025-05-12T20:16:01.933794Z","shell.execute_reply":"2025-05-12T20:16:02.226950Z"},"id":"bd4liREwSkTw","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"%%time\nindex = 10\n\narticle = dataset['test'][index]['article']\nsummary = dataset['test'][index]['highlights']\n\nformatted_prompt = f\"Instruct: Summarize the following article.\\n{article}\\nOutput:\\n\"\nres = gen(original_model, formatted_prompt, 100,)\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:02.228989Z","iopub.execute_input":"2025-05-12T20:16:02.229280Z","iopub.status.idle":"2025-05-12T20:16:06.005604Z","shell.execute_reply.started":"2025-05-12T20:16:02.229256Z","shell.execute_reply":"2025-05-12T20:16:06.004954Z"},"id":"A_gmNFEVSzKB","outputId":"91503dcd-b140-408d-a8e2-ad8f2badf554","trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following article.\nLondon (CNN)A 19-year-old man was charged Wednesday with terror offenses after he was arrested as he returned to Britain from Turkey, London's Metropolitan Police said. Yahya Rashid, a UK national from northwest London, was detained at Luton airport on Tuesday after he arrived on a flight from Istanbul, police said. He's been charged with engaging in conduct in preparation of acts of terrorism, and with engaging in conduct with the intention of assisting others to commit acts of terrorism. Both charges relate to the period between November 1 and March 31. Rashid is due to appear in Westminster Magistrates' Court on Wednesday, police said. CNN's Lindsay Isaac contributed to this report.\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nLondon's Metropolitan Police say the man was arrested at Luton airport after landing on a flight from Istanbul .\nHe's been charged with terror offenses allegedly committed since the start of November .\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nA 19-year-old man from northwest London was arrested at Luton airport on Tuesday after returning to Britain from Turkey. He was charged with engaging in conduct in preparation of acts of terrorism and engaging in conduct with the intention of assisting others to commit acts of terrorism. The charges relate to the period between November 1 and March 31. He is due to appear in Westminster Magistrates' Court on Wednesday.\n\nCPU times: user 3.37 s, sys: 61.8 ms, total: 3.43 s\nWall time: 3.77 s\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# filepath: /Users/madnanrizqu/Code/KAU/cs681-final/summarization/main.ipynb\ndef create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters\n    :param sample: Sample dictionary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below article.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n\n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['article']}\" if sample[\"article\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['highlights']}\"\n    end = f\"{END_KEY}\"\n\n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:16:06.006375Z","iopub.execute_input":"2025-05-12T20:16:06.006622Z","iopub.status.idle":"2025-05-12T20:16:06.011492Z","shell.execute_reply.started":"2025-05-12T20:16:06.006596Z","shell.execute_reply":"2025-05-12T20:16:06.010725Z"},"id":"cr-2z2t_TACP","trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:16:06.012273Z","iopub.execute_input":"2025-05-12T20:16:06.012498Z","iopub.status.idle":"2025-05-12T20:16:06.031129Z","shell.execute_reply.started":"2025-05-12T20:16:06.012471Z","shell.execute_reply":"2025-05-12T20:16:06.030396Z"},"id":"EqV7-1W-TEy5","trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Combine all splits into one dataset\ncombined_dataset = {\n    'train': dataset['train'],\n    'validation': dataset['validation'],\n    'test': dataset['test']\n}\n\n# Get the total size\ntrain_size = len(combined_dataset['train'])\nvalidation_size = len(combined_dataset['validation'])\ntest_size = len(combined_dataset['test'])\ntotal_size = train_size + validation_size + test_size\n\nprint(f\"Total dataset size: {total_size}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:06.032167Z","iopub.execute_input":"2025-05-12T20:16:06.032427Z","iopub.status.idle":"2025-05-12T20:16:06.046235Z","shell.execute_reply.started":"2025-05-12T20:16:06.032406Z","shell.execute_reply":"2025-05-12T20:16:06.045551Z"},"id":"X3AIrn45_k0M","outputId":"5929e9b2-576c-43f6-a5c6-7b3fff345cdf","trusted":true},"outputs":[{"name":"stdout","text":"Total dataset size: 311971\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Calculate new split sizes\nnew_train_size = int(0.70 * total_size)\nnew_val_size = int(0.15 * total_size)\nnew_dev_size = int(0.10 * total_size)\nnew_test_size = int(0.05 * total_size)\n\nprint(f\"New train size: {new_train_size}\")\nprint(f\"New validation size: {new_val_size}\")\nprint(f\"New dev size: {new_dev_size}\")\nprint(f\"New test size: {new_test_size}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:06.046831Z","iopub.execute_input":"2025-05-12T20:16:06.047023Z","iopub.status.idle":"2025-05-12T20:16:06.060617Z","shell.execute_reply.started":"2025-05-12T20:16:06.047009Z","shell.execute_reply":"2025-05-12T20:16:06.060106Z"},"id":"h0x4tWjm_k0M","outputId":"ac4c90d4-a8a9-44a9-ea35-a2662b333b91","trusted":true},"outputs":[{"name":"stdout","text":"New train size: 218379\nNew validation size: 46795\nNew dev size: 31197\nNew test size: 15598\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\n# Concatenate all splits into a single dataset\nall_data = concatenate_datasets([\n    dataset['train'],\n    dataset['validation'],\n    dataset['test']\n])\n\n# Ensure deterministic shuffling using GLOBAL_SEED\nshuffled_indices = np.random.permutation(len(all_data))\nall_data = all_data.select(shuffled_indices)\n\n# Create new splits\nnew_train_size = int(0.70 * len(all_data))\nnew_val_size = int(0.15 * len(all_data))\nnew_dev_size = int(0.10 * len(all_data))\nnew_test_size = int(0.05 * len(all_data))\n\n# Create new splits\nnew_train_dataset = all_data.select(range(0, new_train_size))\nnew_val_dataset = all_data.select(range(new_train_size, new_train_size + new_val_size))\nnew_dev_dataset = all_data.select(range(new_train_size + new_val_size, new_train_size + new_val_size + new_dev_size))\nnew_test_dataset = all_data.select(range(new_train_size + new_val_size + new_dev_size, new_train_size + new_val_size + new_dev_size + new_test_size))\n\nprint(f\"New train dataset size: {len(new_train_dataset)}\")\nprint(f\"New validation dataset size: {len(new_val_dataset)}\")\nprint(f\"New dev dataset size: {len(new_dev_dataset)}\")\nprint(f\"New test dataset size: {len(new_test_dataset)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:06.061343Z","iopub.execute_input":"2025-05-12T20:16:06.061950Z","iopub.status.idle":"2025-05-12T20:16:06.169255Z","shell.execute_reply.started":"2025-05-12T20:16:06.061928Z","shell.execute_reply":"2025-05-12T20:16:06.168652Z"},"id":"u_UzPloJ_k0M","outputId":"ed5f646e-5fff-4b75-edeb-add60963d428","trusted":true},"outputs":[{"name":"stdout","text":"New train dataset size: 218379\nNew validation dataset size: 46795\nNew dev dataset size: 31197\nNew test dataset size: 15598\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from functools import partial\n\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n\n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)\n\n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'article', 'highlights'],  # Updated column names\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:16:06.169929Z","iopub.execute_input":"2025-05-12T20:16:06.170122Z","iopub.status.idle":"2025-05-12T20:16:06.175132Z","shell.execute_reply.started":"2025-05-12T20:16:06.170106Z","shell.execute_reply":"2025-05-12T20:16:06.174385Z"},"id":"khyBNnMOTNYY","trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:06.175851Z","iopub.execute_input":"2025-05-12T20:16:06.176084Z","iopub.status.idle":"2025-05-12T20:16:06.192002Z","shell.execute_reply.started":"2025-05-12T20:16:06.176068Z","shell.execute_reply":"2025-05-12T20:16:06.191295Z"},"id":"VypP9B9MTT-Q","outputId":"05379bf9-3959-4d2b-e63b-397b83d8e189","trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 3208 MB.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Preprocess all splits\nmax_length = get_max_length(original_model)\nprint(f\"Using max length: {max_length}\")\n\n# Process each split using the global seed\ntrain_dataset = preprocess_dataset(tokenizer, max_length, GLOBAL_SEED, new_train_dataset.select(range(210)))\neval_dataset = preprocess_dataset(tokenizer, max_length, GLOBAL_SEED, new_val_dataset.select(range(45)))\ndev_dataset = preprocess_dataset(tokenizer, max_length, GLOBAL_SEED, new_dev_dataset.select(range(30)))\ntest_dataset = preprocess_dataset(tokenizer, max_length, GLOBAL_SEED, new_test_dataset.select(range(15)))\n\n# Print dataset shapes\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(f\"Dev: {dev_dataset.shape}\")\nprint(f\"Test: {test_dataset.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:06.192862Z","iopub.execute_input":"2025-05-12T20:16:06.193081Z","iopub.status.idle":"2025-05-12T20:16:12.547396Z","shell.execute_reply.started":"2025-05-12T20:16:06.193066Z","shell.execute_reply":"2025-05-12T20:16:12.546779Z"},"id":"Fn5T8UCPTXqd","outputId":"b36b18d5-ea3d-434e-cba4-b1eeeec02762","trusted":true},"outputs":[{"name":"stdout","text":"Found max lenth: 2048\nUsing max length: 2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/210 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4872ec4e5e34dbfabdc31e01a0048b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/210 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01dc1024f0ac459589b38507627a34fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/210 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925e96d7219045d99acf87af67413070"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cce4bce8576749b9890ed6133d2dda75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fadeaebcf8b4c8ca658d0f2eaebcc57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/45 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cfb3f2ab9104e9e9584b334b25a490b"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff350f64da64297bf01f8bcf1cd6520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f029b2dc99d4938a9c8e34772185cc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7dee105870470d899537a01807cdad"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"115e4375b5ca483eb8ea10b3dd9fc327"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82aa607a4d5b42f4b825111bbeb88344"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/15 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a666bc2ca9e44f39db4e6377d746c19"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: (203, 3)\nValidation: (44, 3)\nDev: (28, 3)\nTest: (15, 3)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:12.548113Z","iopub.execute_input":"2025-05-12T20:16:12.548392Z","iopub.status.idle":"2025-05-12T20:16:12.552899Z","shell.execute_reply.started":"2025-05-12T20:16:12.548368Z","shell.execute_reply":"2025-05-12T20:16:12.552166Z"},"id":"G9rpKh-YTwSO","outputId":"6cdfcbed-56ff-4ef7-c26b-8bdabb2a4465","trusted":true},"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (203, 3)\nValidation: (44, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 203\n})\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:12.553679Z","iopub.execute_input":"2025-05-12T20:16:12.554014Z","iopub.status.idle":"2025-05-12T20:16:12.575255Z","shell.execute_reply.started":"2025-05-12T20:16:12.553997Z","shell.execute_reply":"2025-05-12T20:16:12.574494Z"},"id":"yuoK0dCkT0az","outputId":"0a7e60de-ce3c-45e2-8bca-10afe480df43","trusted":true},"outputs":[{"name":"stdout","text":"trainable model parameters: 262364160\nall model parameters: 1521392640\npercentage of trainable model parameters: 17.24%\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(original_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:12.576242Z","iopub.execute_input":"2025-05-12T20:16:12.576479Z","iopub.status.idle":"2025-05-12T20:16:12.591746Z","shell.execute_reply.started":"2025-05-12T20:16:12.576446Z","shell.execute_reply":"2025-05-12T20:16:12.590966Z"},"id":"IOm68qZZT4q8","outputId":"ebc0fc75-f765-4729-e10e-7874d10da5e0","trusted":true},"outputs":[{"name":"stdout","text":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (rotary_emb): PhiRotaryEmbedding()\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:16:12.592893Z","iopub.execute_input":"2025-05-12T20:16:12.593188Z","iopub.status.idle":"2025-05-12T20:16:12.969260Z","shell.execute_reply.started":"2025-05-12T20:16:12.593172Z","shell.execute_reply":"2025-05-12T20:16:12.968686Z"},"id":"djYOCCntT7rK","trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:12.969994Z","iopub.execute_input":"2025-05-12T20:16:12.970220Z","iopub.status.idle":"2025-05-12T20:16:12.977593Z","shell.execute_reply.started":"2025-05-12T20:16:12.970203Z","shell.execute_reply":"2025-05-12T20:16:12.976888Z"},"id":"x8gOrr9IUG8X","outputId":"07b801c3-1ba4-4e24-976b-e569196866b6","trusted":true},"outputs":[{"name":"stdout","text":"trainable model parameters: 20971520\nall model parameters: 1542364160\npercentage of trainable model parameters: 1.36%\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:12.978269Z","iopub.execute_input":"2025-05-12T20:16:12.978464Z","iopub.status.idle":"2025-05-12T20:16:12.995027Z","shell.execute_reply.started":"2025-05-12T20:16:12.978450Z","shell.execute_reply":"2025-05-12T20:16:12.994435Z"},"id":"i27yzRadUKUN","outputId":"6a2c6de8-b212-4795-bf38-18b233844c2c","trusted":true},"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PhiForCausalLM(\n      (model): PhiModel(\n        (embed_tokens): Embedding(51200, 2560)\n        (layers): ModuleList(\n          (0-31): 32 x PhiDecoderLayer(\n            (self_attn): PhiAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (dense): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): PhiMLP(\n              (activation_fn): NewGELUActivation()\n              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n            )\n            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (rotary_emb): PhiRotaryEmbedding()\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=10,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=10,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=10,\n    num_train_epochs=1,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n    seed=GLOBAL_SEED,  # Add global seed\n    data_seed=GLOBAL_SEED  # Add data seed for data loaders\n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:12.998184Z","iopub.execute_input":"2025-05-12T20:16:12.998357Z","iopub.status.idle":"2025-05-12T20:16:13.044635Z","shell.execute_reply.started":"2025-05-12T20:16:12.998344Z","shell.execute_reply":"2025-05-12T20:16:13.044090Z"},"id":"ijGOIAJAUQjK","outputId":"b361a160-805e-4d39-9469-95f1835e824f","trusted":true},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"peft_training_args.device","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-12T20:16:13.045257Z","iopub.execute_input":"2025-05-12T20:16:13.045425Z","iopub.status.idle":"2025-05-12T20:16:13.050125Z","shell.execute_reply.started":"2025-05-12T20:16:13.045412Z","shell.execute_reply":"2025-05-12T20:16:13.049489Z"},"id":"koz0-4PHUZzX","outputId":"8439479e-1eed-4b12-86b0-c539e6cc6fef","trusted":true},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2025-05-12T20:16:13.050745Z","iopub.execute_input":"2025-05-12T20:16:13.051004Z","iopub.status.idle":"2025-05-12T20:19:47.571204Z","shell.execute_reply.started":"2025-05-12T20:16:13.050984Z","shell.execute_reply":"2025-05-12T20:19:47.570617Z"},"id":"FFcxA1B-UfJ1","outputId":"61ea5329-96a8-47f7-d188-f68db5d163fb","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 03:08, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.437600</td>\n      <td>2.414258</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=2.4376129150390624, metrics={'train_runtime': 213.865, 'train_samples_per_second': 0.187, 'train_steps_per_second': 0.047, 'total_flos': 667241004165120.0, 'train_loss': 2.4376129150390624, 'epoch': 0.19704433497536947})"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:19:47.571958Z","iopub.execute_input":"2025-05-12T20:19:47.572237Z","iopub.status.idle":"2025-05-12T20:19:47.576632Z","shell.execute_reply.started":"2025-05-12T20:19:47.572213Z","shell.execute_reply":"2025-05-12T20:19:47.575857Z"},"id":"QajO3VMeXWAV","trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 14830 MB.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Step 8: Evaluate on Dev Split\nprint(\"Evaluating on Dev split...\")\ndev_results = peft_trainer.evaluate(dev_dataset)\nprint(f\"Dev set evaluation results:\")\nprint(dev_results)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:19:47.577404Z","iopub.execute_input":"2025-05-12T20:19:47.577579Z","iopub.status.idle":"2025-05-12T20:20:31.344766Z","shell.execute_reply.started":"2025-05-12T20:19:47.577564Z","shell.execute_reply":"2025-05-12T20:20:31.344110Z"},"id":"oWuUsgZY_k0O","trusted":true},"outputs":[{"name":"stdout","text":"Evaluating on Dev split...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:59]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Dev set evaluation results:\n{'eval_loss': 2.339982748031616, 'eval_runtime': 43.7269, 'eval_samples_per_second': 0.64, 'eval_steps_per_second': 0.091, 'epoch': 0.19704433497536947}\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Step 9: Final Evaluation on Test Split (only run this once training is complete)\nprint(\"Performing final evaluation on Test split...\")\ntest_results = peft_trainer.evaluate(test_dataset)\nprint(f\"Final test set evaluation results:\")\nprint(test_results)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:20:31.345519Z","iopub.execute_input":"2025-05-12T20:20:31.345745Z","iopub.status.idle":"2025-05-12T20:21:02.320752Z","shell.execute_reply.started":"2025-05-12T20:20:31.345721Z","shell.execute_reply":"2025-05-12T20:21:02.319813Z"},"id":"ZgraAjRO_k0O","trusted":true},"outputs":[{"name":"stdout","text":"Performing final evaluation on Test split...\nFinal test set evaluation results:\n{'eval_loss': 2.446943998336792, 'eval_runtime': 30.9478, 'eval_samples_per_second': 0.485, 'eval_steps_per_second': 0.065, 'epoch': 0.19704433497536947}\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Free memory for merging weights\nif peft_trainer:\n  del peft_trainer\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:21:02.321726Z","iopub.execute_input":"2025-05-12T20:21:02.322008Z","iopub.status.idle":"2025-05-12T20:21:02.550568Z","shell.execute_reply.started":"2025-05-12T20:21:02.321987Z","shell.execute_reply":"2025-05-12T20:21:02.549967Z"},"id":"34AMrQQ-ZXcl","trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:21:02.551326Z","iopub.execute_input":"2025-05-12T20:21:02.551553Z","iopub.status.idle":"2025-05-12T20:21:02.556055Z","shell.execute_reply.started":"2025-05-12T20:21:02.551536Z","shell.execute_reply":"2025-05-12T20:21:02.555341Z"},"id":"WWvkFhcYZa4H","trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 3712 MB.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"%%time\n\nindex = 11\n\narticle = new_test_dataset[index]['article']\nsummary = new_test_dataset[index]['highlights']\n\nprompt = f\"Instruct: Summarize the following article.\\n{article}\\nOutput:\\n\"\n\npeft_model_res = gen(peft_model, prompt, 100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{peft_model_output}')","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:21:02.556757Z","iopub.execute_input":"2025-05-12T20:21:02.557013Z","iopub.status.idle":"2025-05-12T20:21:06.405115Z","shell.execute_reply.started":"2025-05-12T20:21:02.556997Z","shell.execute_reply":"2025-05-12T20:21:06.404181Z"},"id":"KqWw9S22ahPZ","trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following article.\nBy . Kerry Mcdermott . PUBLISHED: . 09:55 EST, 24 April 2013 . | . UPDATED: . 10:57 EST, 25 April 2013 . Fancy joining the mile-high club? Now you can make your intentions clear without leaving your seat - even when the 'fasten seatbelt sign is on'. A new Virgin gimmick lets you buy your fellow passenger a drink via the seat-back entertainment system, allowing for full-on in-flight flirting. The America service lets flyers locate the object of their affection via a digital seat map, then send them a tipple at the touch of a button. Scroll down for video . Cheers! Virgin passengers can locate the object of their affections via a digital seat map in the seat-back system in front of them . Target the object of your affection: Tycoon Sir Richard Branson has released a 'guide to getting lucky' by using the system . Flyers first locate . the object of their affection via a digital seat map. They then select what they want to send, be it drinks, snacks or even a meal, and enter their credit card details. Staff then deliver the food to the appropriate seat. Customers can then follow up their gift via a seat to seat chat system . After selecting items and paying with a credit card, a flight attendant delivers the goodies directly to the passenger's seat. They can even follow up the gesture with a chat-up line sent through the seat-to-seat messaging system. Passengers can also have meals and snacks delivered to people sitting elsewhere on the aircraft - so if things go well an admirer could end up buying you dinner. After the delivery, you can follow up and chat with your object of affection with Virgin America's existing seat-to-seat chat platform via its Red in-flight entertainment system. The chat platform allows travelers to send text messages to other fliers. Billionaire Virgin boss Richard Branson . unveiled the new service to the in-flight entertainment system with a . video entitled: 'Sir Richard Branson's Guide to Getting Lucky at 35,000 . feet'. 'I'm not a betting man, but I say your chance of deplaning with a plus-one are at least 50 percent,' he told potential customers. Hey Girl! Sir Richard has given would-be passengers advice on how to be as successful as him . Unleash your charm: Sir Richard's tips for mile-high fun are included in a promotional video to go with the seat-to-seat deilvery system . Buy her a drink: As well as encouraging passengers to get cosy on the flight, Sir Richard's video encourages suitors to spend cash on alcohol and even dinner for one another . The tycoon uses the tongue-in-cheek video message to advise his passengers to 'pinpoint the object of your affections', then woo them by sending them a drink or striking up a seat-to-seat chat. In is the latest in a series of stunts the Virgin boss has used to gain publicity for his ventures, from flashing his underwear to abseiling down buildings. Branson, who signs off by saying . 'best of luck up there', launched the new feature to mark the start of . Virgin's new Los Angeles to Las Vegas service. It is available on all of the airline's U.S. flights. Fasten your seatbelts: The new service is available on all of Virgin's U.S. flights .\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nPassengers can select their chosen recipient via digital seat map .\nCan send a drink to somebody seated anywhere else on aircraft .\nNew feature of Virgin's in-flight entertainment service .\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\nA new Virgin gimmick lets flyers locate the object of their affection via a digital seat map in the seat-back system in front of them. They can then send them a tipple at the touch of a button.\n\nCPU times: user 3.65 s, sys: 180 ms, total: 3.83 s\nWall time: 3.83 s\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import pandas as pd\nimport evaluate\nimport torch\nimport numpy as np\n\narticles = new_test_dataset[15:18]['article']\nhuman_baseline_summaries = new_test_dataset[15:18]['highlights']\n\noriginal_model_summaries = []\npeft_model_summaries = []\n\nfor idx, article in enumerate(articles):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following article.\\n{article}\\nOutput:\\n\"\n\n    original_model_res = gen(original_model, prompt, 100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n\n    peft_model_res = gen(peft_model, prompt, 100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n\ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:21:06.405836Z","iopub.execute_input":"2025-05-12T20:21:06.406095Z","iopub.status.idle":"2025-05-12T20:21:40.918801Z","shell.execute_reply.started":"2025-05-12T20:21:06.406068Z","shell.execute_reply":"2025-05-12T20:21:40.917948Z"},"id":"EIPSiTRWbJs3","trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  France announces new ministers .\\nThe governme...   \n1  Miss California USA Carrie Prejean says she po...   \n2  Jay-Z reported to be 'deeply disappointed' by ...   \n\n                            original_model_summaries  \\\n0  A new French government was announced, with Em...   \n1  A second lingerie-modeling photo of Miss Calif...   \n2  Rihanna's friends are concerned about her reco...   \n\n                                peft_model_summaries  \n0  A new French government was announced, with Em...  \n1  A second lingerie-modeling photo of Miss Calif...  \n2  Rihanna's friends are concerned about her reco...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>France announces new ministers .\\nThe governme...</td>\n      <td>A new French government was announced, with Em...</td>\n      <td>A new French government was announced, with Em...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Miss California USA Carrie Prejean says she po...</td>\n      <td>A second lingerie-modeling photo of Miss Calif...</td>\n      <td>A second lingerie-modeling photo of Miss Calif...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jay-Z reported to be 'deeply disappointed' by ...</td>\n      <td>Rihanna's friends are concerned about her reco...</td>\n      <td>Rihanna's friends are concerned about her reco...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"import evaluate\n# ROUGE evaluation\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\n# BLEU evaluation\nbleu = evaluate.load('bleu')\n\noriginal_model_bleu = bleu.compute(\n    predictions=original_model_summaries,\n    references=[[ref] for ref in human_baseline_summaries]\n)\n\npeft_model_bleu = bleu.compute(\n    predictions=peft_model_summaries, \n    references=[[ref] for ref in human_baseline_summaries]\n)\n\n# Fixed perplexity calculation function\ndef calculate_perplexity_alt(model, tokenizer, texts):\n    perplexities = []\n    \n    with torch.no_grad():\n        for text in texts:\n            # Tokenize without setting labels\n            inputs = tokenizer(text, return_tensors=\"pt\")\n            # Move to device\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n            \n            # Forward pass to get logits\n            outputs = model(**inputs)\n            logits = outputs.logits\n            \n            # Calculate loss manually\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = inputs['input_ids'][..., 1:].contiguous()\n            \n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), \n                           shift_labels.reshape(-1))\n            \n            # Calculate perplexity\n            perplexity = torch.exp(loss)\n            perplexities.append(perplexity.item())\n            \n    return sum(perplexities) / len(perplexities)\n\n# Try the alternative calculation\noriginal_perplexity = calculate_perplexity_alt(original_model, eval_tokenizer, human_baseline_summaries)\npeft_perplexity = calculate_perplexity_alt(peft_model, eval_tokenizer, peft_model_summaries)\n\n# Helper function for formatting outputs\ndef format_metrics(rouge_results, bleu_results, perplexity):\n    print(\"=== ROUGE Scores ===\")\n    for metric, value in rouge_results.items():\n        print(f\"  {metric}: {value*100:.2f}%\")\n    \n    print(\"\\n=== BLEU Score ===\")\n    print(f\"  BLEU: {bleu_results['bleu']*100:.2f}%\")\n    print(\"  Precision by n-gram:\")\n    for n, precision in enumerate(bleu_results['precisions'], 1):\n        print(f\"    {n}-gram: {precision*100:.2f}%\")\n    \n    print(f\"\\n=== Perplexity ===\")\n    print(f\"  {perplexity:.4f} (lower is better)\")\n\n# Print results with better formatting\nprint(\"\\n\" + \"=\"*50)\nprint(\"ORIGINAL MODEL EVALUATION\")\nprint(\"=\"*50)\nformat_metrics(original_model_results, original_model_bleu, original_perplexity)\n\nprint(\"\\n\\n\" + \"=\"*50)\nprint(\"PEFT MODEL EVALUATION\")\nprint(\"=\"*50)\nformat_metrics(peft_model_results, peft_model_bleu, peft_perplexity)\n\n# Calculate and display improvements\nprint(\"\\n\\n\" + \"=\"*50)\nprint(\"IMPROVEMENT SUMMARY\")\nprint(\"=\"*50)\n\n# ROUGE improvement\nprint(\"=== ROUGE Improvement ===\")\nfor metric in original_model_results.keys():\n    improvement = (peft_model_results[metric] - original_model_results[metric]) * 100\n    print(f\"  {metric}: {improvement:.2f}% absolute improvement\")\n\n# BLEU improvement\nbleu_improvement = (peft_model_bleu['bleu'] - original_model_bleu['bleu']) * 100\nprint(\"\\n=== BLEU Improvement ===\")\nprint(f\"  {bleu_improvement:.2f}% absolute improvement\")\n\n# Perplexity improvement (lower is better)\nperplexity_improvement = original_perplexity - peft_perplexity\nperplexity_improvement_percent = (perplexity_improvement / original_perplexity) * 100\nprint(\"\\n=== Perplexity Improvement ===\")\nprint(f\"  {perplexity_improvement_percent:.2f}% reduction\")","metadata":{"execution":{"iopub.status.busy":"2025-05-12T20:37:04.608685Z","iopub.execute_input":"2025-05-12T20:37:04.609390Z","iopub.status.idle":"2025-05-12T20:37:08.282645Z","shell.execute_reply.started":"2025-05-12T20:37:04.609366Z","shell.execute_reply":"2025-05-12T20:37:08.282016Z"},"id":"4VLiakHMbu_U","trusted":true},"outputs":[{"name":"stdout","text":"\n==================================================\nORIGINAL MODEL EVALUATION\n==================================================\n=== ROUGE Scores ===\n  rouge1: 20.63%\n  rouge2: 6.75%\n  rougeL: 13.85%\n  rougeLsum: 16.11%\n\n=== BLEU Score ===\n  BLEU: 3.97%\n  Precision by n-gram:\n    1-gram: 22.10%\n    2-gram: 5.62%\n    3-gram: 1.71%\n    4-gram: 1.16%\n\n=== Perplexity ===\n  77.4162 (lower is better)\n\n\n==================================================\nPEFT MODEL EVALUATION\n==================================================\n=== ROUGE Scores ===\n  rouge1: 22.66%\n  rouge2: 9.18%\n  rougeL: 16.66%\n  rougeLsum: 19.13%\n\n=== BLEU Score ===\n  BLEU: 5.19%\n  Precision by n-gram:\n    1-gram: 26.14%\n    2-gram: 7.33%\n    3-gram: 2.72%\n    4-gram: 1.39%\n\n=== Perplexity ===\n  17.3879 (lower is better)\n\n\n==================================================\nIMPROVEMENT SUMMARY\n==================================================\n=== ROUGE Improvement ===\n  rouge1: 2.02% absolute improvement\n  rouge2: 2.43% absolute improvement\n  rougeL: 2.80% absolute improvement\n  rougeLsum: 3.02% absolute improvement\n\n=== BLEU Improvement ===\n  1.22% absolute improvement\n\n=== Perplexity Improvement ===\n  77.54% reduction\n","output_type":"stream"}],"execution_count":45}]}